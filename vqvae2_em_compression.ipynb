{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EM Compression — Demo Overview\n",
        "\n",
        "Below is the demo overview for three EM (electron microscopy) compression / reconstruction modes. Each mode targets a different balance of compression ratio, segmentation reliability, and fine-structure fidelity.\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Summary\n",
        "\n",
        "| Mode | Default Compression | Best For | Key Behavior | Known Artifacts | Extra Training |\n",
        "|---|---:|---|---|---|---|\n",
        "| 1) **Top-Only Training / Reconstruction** | **1024×** | Whole-cell segmentation without a trained prior | Membrane prediction remains stable even at extreme compression | Occasional **2D membrane breaks**; requires **3D repair** | None |\n",
        "| 2) **Two-Level VQ-VAE** | **204×** | Preserving overall cellular **texture** with moderate compression | Good global appearance retention | Possible **vesicle** shape deformation | Optional **cross-attention prior** to correct deformation |\n",
        "| 3) **Two-Level VQ-VAE + Transformer Prior** | **1024×** | High compression **with more detail** restored | **Top-VQ tokens** condition the prediction of **Bottom-VQ tokens**; bottom level fills details | Higher training cost | Requires **Transformer prior**; pairs well with Mode-2 **teacher net** |\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Top-Only Training / Reconstruction (1024×)\n",
        "\n",
        "- **Use when:** You do **not** have a trained prior network and want **extreme compression** to segment entire cells while ignoring most intracellular detail.\n",
        "- **Behavior:** Cell membrane predictions are generally stable at **1024×**.\n",
        "- **Limitation:** **Membrane breaks in 2D slices** can occur.\n",
        "- **Mitigation:** Perform **3D reconstruction / consistency repair** (e.g., connect components across slices) to restore continuity.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Two-Level VQ-VAE (204×)\n",
        "\n",
        "- **Use when:** You need **overall texture preservation** with substantial compression.\n",
        "- **Behavior:** Maintains global cellular appearance at ~**204×**.\n",
        "- **Limitation:** May introduce **vesicle** (not “vehicle”) deformation artifacts.\n",
        "- **Mitigation:** Train a **targeted prior** with **cross-attention** to correct local shape issues **without changing the compression ratio**.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Two-Level VQ-VAE + Transformer Prior (1024×)\n",
        "\n",
        "- **Use when:** You want **1024× compression** **and** to **restore more details** than Mode 1.\n",
        "- **Idea:** Train a **Transformer prior** to **predict Bottom-VQ tokens conditioned on Top-VQ tokens**. Treat **Top-VQ** as the coarse **backbone**, and use **Bottom-VQ** to **fill in details**.\n",
        "- **Training:** Requires an additional **Transformer**. For best results, **pair with the teacher/prior network** from Mode 2.\n",
        "\n",
        "---\n",
        "\n",
        "## Notes & Recommendations\n",
        "\n",
        "- **3D repair is essential** for Mode 1 to address 2D membrane breaks.\n",
        "- If **vesicle deformation** is unacceptable at 204× (Mode 2), add a **cross-attention prior** tailored to vesicle morphology.\n",
        "- For **max compression with detail** (Mode 3), the **Transformer prior** uses **Top-VQ** as context to recover **Bottom-VQ**; combining it with the **teacher net** from Mode 2 typically improves perceptual quality and segmentation consistency.\n",
        "\n",
        "---\n",
        "\n",
        "## Evaluation & Repro Tips\n",
        "\n",
        "- Fix random seeds for comparability.\n",
        "- Report both **perceptual** metrics (e.g., SSIM/LPIPS) and **task** metrics (e.g., membrane F1, segmentation accuracy).\n",
        "- Always evaluate **in 3D** when the downstream task is 3D segmentation.\n",
        "\n",
        "---\n",
        "\n",
        "## Glossary (brief)\n",
        "\n",
        "- **Top-VQ / Bottom-VQ:** Codebooks from the two-level VQ-VAE; **Top-VQ** captures coarse structure (context), **Bottom-VQ** refines local detail.\n",
        "- **Prior (Transformer / Cross-attention):** A learned model used to predict or refine discrete VQ token sequences to improve reconstruction quality at fixed bitrate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Top-Only Training / Reconstruction (1024×)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZcrBz9Vhgk1"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# VQ-VAE Top-Only (32x32 latent, EMA codebook) — End-to-End Pipeline\n",
        "# - Reads dataset zip from Google Drive (compression_em.zip)\n",
        "# - Unzips locally, sorts by filename, first 400 for training, last 100 for prediction\n",
        "# - Handles large images (~2k x 4k) by tiling into 1024x1024 patches\n",
        "# - Trains, exports bundle, reconstructs full-res predictions by stitching tiles\n",
        "# ==============================================================\n",
        "\n",
        "import os, time, json, math, zipfile, shutil, glob, re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# -----------------------------\n",
        "# (0) Global Hyperparams\n",
        "# -----------------------------\n",
        "IMAGE_SIZE       = int(globals().get('IMAGE_SIZE', 1024))   # tile size for training/inference\n",
        "TILE             = IMAGE_SIZE\n",
        "TOP_GRID         = int(globals().get('TOP_GRID', 32))       # latent grid (32x32)\n",
        "\n",
        "NUM_HIDDENS_TOP    = int(globals().get('NUM_HIDDENS_TOP', 128))\n",
        "EMBEDDING_DIM_TOP  = int(globals().get('EMBEDDING_DIM_TOP', 96))\n",
        "NUM_EMBEDDINGS_TOP = int(globals().get('NUM_EMBEDDINGS_TOP', 256))\n",
        "COMMITMENT_COST_TOP= float(globals().get('COMMITMENT_COST_TOP', 1.0))\n",
        "\n",
        "BATCH_SIZE       = int(globals().get('BATCH_SIZE', 2))\n",
        "EPOCHS_WARMUP    = int(globals().get('EPOCHS_WARMUP', 100))\n",
        "LR_WARMUP        = float(globals().get('LR_WARMUP', 2e-4))\n",
        "WEIGHT_DECAY     = float(globals().get('WEIGHT_DECAY', 1e-4))\n",
        "\n",
        "# Drive dataset zip path (adjust if different)\n",
        "DRIVE_ZIP = \"/content/drive/MyDrive/compression_em.zip\"\n",
        "LOCAL_ZIP = \"/content/compression_em.zip\"\n",
        "EXTRACT_DIR = \"/content/data/compression_em\"\n",
        "\n",
        "# -----------------------------\n",
        "# (A) Residual helpers\n",
        "# -----------------------------\n",
        "\n",
        "def residual_block(x, filters):\n",
        "    skip = x\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters//2, 3, padding='same')(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv2D(filters, 1, padding='same')(x)\n",
        "    return tf.keras.layers.Add()([skip, x])\n",
        "\n",
        "\n",
        "def residual_stack(x, filters, num_blocks=2):\n",
        "    for _ in range(num_blocks):\n",
        "        x = residual_block(x, filters)\n",
        "    return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "# -----------------------------\n",
        "# (B) EMA VectorQuantizer\n",
        "# -----------------------------\n",
        "class VectorQuantizerEMA(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_embeddings, embedding_dim,\n",
        "                 commitment_cost=0.25, decay=0.99, epsilon=1e-5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_embeddings = int(num_embeddings)\n",
        "        self.embedding_dim = int(embedding_dim)\n",
        "        self.commitment_cost = float(commitment_cost)\n",
        "        self.decay = float(decay)\n",
        "        self.epsilon = float(epsilon)\n",
        "        self._perplexity = tf.keras.metrics.Mean(name=f'{self.name}_perplexity')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self._perplexity]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            name='embeddings',\n",
        "            shape=(self.embedding_dim, self.num_embeddings),\n",
        "            initializer=tf.keras.initializers.RandomUniform(-1.0/self.num_embeddings, 1.0/self.num_embeddings),\n",
        "            trainable=False\n",
        "        )\n",
        "        self.ema_cluster_size = self.add_weight(\n",
        "            name='ema_cluster_size', shape=(self.num_embeddings,), initializer='zeros', trainable=False)\n",
        "        self.ema_dw = self.add_weight(\n",
        "            name='ema_dw', shape=(self.embedding_dim, self.num_embeddings), initializer='zeros', trainable=False)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        shp  = tf.shape(inputs)                              # [B,H,W,C]\n",
        "        flat = tf.reshape(inputs, [-1, self.embedding_dim])  # [N,C]\n",
        "\n",
        "        dists = (tf.reduce_sum(flat**2, axis=1, keepdims=True)\n",
        "                 - 2.0 * tf.matmul(flat, self.embeddings)\n",
        "                 + tf.reduce_sum(self.embeddings**2, axis=0, keepdims=True))  # [N,K]\n",
        "        idx  = tf.argmax(-dists, axis=1)                     # [N]\n",
        "        one  = tf.one_hot(idx, self.num_embeddings, dtype=flat.dtype)  # [N,K]\n",
        "        quant = tf.matmul(one, tf.transpose(self.embeddings))          # [N,C]\n",
        "        quant = tf.reshape(quant, shp)                                   # [B,H,W,C]\n",
        "\n",
        "        def _ema_update():\n",
        "            cluster_size = tf.reduce_sum(one, axis=0)\n",
        "            dw = tf.matmul(tf.transpose(flat), one)\n",
        "            ema_cs = self.ema_cluster_size * self.decay + cluster_size * (1.0 - self.decay)\n",
        "            ema_dw = self.ema_dw * self.decay + dw * (1.0 - self.decay)\n",
        "            n = tf.reduce_sum(ema_cs)\n",
        "            smoothed_cs = (ema_cs + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n\n",
        "            new_embed = ema_dw / tf.expand_dims(smoothed_cs, 0)\n",
        "            self.ema_cluster_size.assign(ema_cs)\n",
        "            self.ema_dw.assign(ema_dw)\n",
        "            self.embeddings.assign(new_embed)\n",
        "            return 0.0\n",
        "\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "        tf.cond(tf.cast(training, tf.bool), _ema_update, lambda: 0.0)\n",
        "\n",
        "        # commitment loss\n",
        "        e_loss = tf.reduce_mean((tf.stop_gradient(quant) - inputs)**2)\n",
        "        self.add_loss(self.commitment_cost * e_loss)\n",
        "\n",
        "        # perplexity\n",
        "        avg_p = tf.reduce_mean(one, axis=0)\n",
        "        perp  = tf.exp(-tf.reduce_sum(avg_p * tf.math.log(avg_p + 1e-10)))\n",
        "        self._perplexity.update_state(perp)\n",
        "\n",
        "        # straight-through\n",
        "        quant = inputs + tf.stop_gradient(quant - inputs)\n",
        "        return quant\n",
        "\n",
        "# -----------------------------\n",
        "# (C) Encoder/Decoder builders\n",
        "# -----------------------------\n",
        "\n",
        "def build_encoder_to_top(img_size, top_grid, num_hiddens):\n",
        "    \"\"\"Downsample 1024->32 via 5x stride-2.\"\"\"\n",
        "    assert (img_size % top_grid) == 0\n",
        "    n_downs = int(math.log2(img_size // top_grid))  # 1024->32 = 5\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, 1))\n",
        "    x = inputs\n",
        "    chans = [64, 128, 128, 128, num_hiddens][:n_downs]\n",
        "    for c in chans:\n",
        "        x = tf.keras.layers.Conv2D(c, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    x = residual_stack(x, num_hiddens, num_blocks=2)\n",
        "    return tf.keras.Model(inputs, x, name=f'encoder_to_top_{top_grid}')\n",
        "\n",
        "\n",
        "def build_decoder_top_to_image(img_size, top_grid, in_ch):\n",
        "    \"\"\"Upsample 32->1024 via 5x deconv.\"\"\"\n",
        "    assert (img_size % top_grid) == 0\n",
        "    n_ups = int(math.log2(img_size // top_grid))    # 32->1024 = 5\n",
        "    inputs = tf.keras.Input(shape=(top_grid, top_grid, in_ch))\n",
        "    x = inputs\n",
        "    ups_ch = [256, 128, 128, 64, 32][:n_ups]\n",
        "    for c in ups_ch:\n",
        "        x = tf.keras.layers.Conv2DTranspose(c, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    out = tf.keras.layers.Conv2D(1, 3, padding='same')(x)\n",
        "    return tf.keras.Model(inputs, out, name=f'decoder_top_{top_grid}_to_{IMAGE_SIZE}')\n",
        "\n",
        "# -----------------------------\n",
        "# (D) Assemble Model\n",
        "# -----------------------------\n",
        "enc_top  = build_encoder_to_top(IMAGE_SIZE, TOP_GRID, NUM_HIDDENS_TOP)\n",
        "pre_vq_top   = tf.keras.layers.Conv2D(EMBEDDING_DIM_TOP, 1, name='pre_vq_top')\n",
        "vq_top       = VectorQuantizerEMA(NUM_EMBEDDINGS_TOP, EMBEDDING_DIM_TOP,\n",
        "                                  commitment_cost=COMMITMENT_COST_TOP, decay=0.99, epsilon=1e-5, name='vq_top')\n",
        "post_vq_top  = tf.keras.layers.Conv2D(NUM_HIDDENS_TOP, 1, name='post_vq_top')\n",
        "dec_top = build_decoder_top_to_image(IMAGE_SIZE, TOP_GRID, in_ch=NUM_HIDDENS_TOP)\n",
        "\n",
        "class VQTopOnlyEMA(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc_top   = enc_top\n",
        "        self.pre_vq_top= pre_vq_top\n",
        "        self.vq_top    = vq_top\n",
        "        self.post_vq_top = post_vq_top\n",
        "        self.dec_top   = dec_top\n",
        "        self.norm_t    = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "        self.drop_t    = tf.keras.layers.SpatialDropout2D(0.1)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        ht  = self.enc_top(x)                   # [B,32,32,Ht]\n",
        "        zt  = self.pre_vq_top(ht)               # [B,32,32,Ct]\n",
        "        ztq = self.vq_top(zt, training=training)\n",
        "        ztq = self.post_vq_top(ztq)             # [B,32,32,Ht]\n",
        "        ztq = self.norm_t(ztq)\n",
        "        ztq = self.drop_t(ztq, training=training)\n",
        "        y   = self.dec_top(ztq)                 # [B,1024,1024,1]\n",
        "        return y\n",
        "\n",
        "model = VQTopOnlyEMA()\n",
        "\n",
        "# -----------------------------\n",
        "# (E) Compile\n",
        "# -----------------------------\n",
        "\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.image.ssim(y_true + 0.5, y_pred + 0.5, max_val=1.0))\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.image.psnr(y_true + 0.5, y_pred + 0.5, max_val=1.0))\n",
        "\n",
        "def compile_warmup(m, lr=LR_WARMUP, wd=WEIGHT_DECAY):\n",
        "    try:\n",
        "        opt = tf.keras.optimizers.AdamW(learning_rate=lr, weight_decay=wd, beta_1=0.9, beta_2=0.95)\n",
        "    except Exception:\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.95)\n",
        "    m.compile(optimizer=opt, loss='mae', metrics=[psnr_metric, ssim_metric])\n",
        "\n",
        "compile_warmup(model)\n",
        "\n", 
        "# -----------------------------\n",
        "# (F) Data IO: unzip -> split -> tf.data with tiling\n",
        "# -----------------------------\n",
        "\n",
        "def _natural_key(s):\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', os.path.basename(s))]\n",
        "\n",
        "# Copy zip from Drive -> local to speed up reads\n",
        "os.makedirs(os.path.dirname(LOCAL_ZIP), exist_ok=True)\n",
        "shutil.copy(DRIVE_ZIP, LOCAL_ZIP)\n",
        "\n",
        "# Unzip locally\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "with zipfile.ZipFile(LOCAL_ZIP, 'r') as zf:\n",
        "    zf.extractall(EXTRACT_DIR)\n",
        "\n",
        "# Collect image files\n",
        "IMG_EXTS = (\".png\", \".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".PNG\", \".tif\", \".tiff\", \".TIF\", \".TIFF\")\n",
        "all_files = [p for p in glob.glob(os.path.join(EXTRACT_DIR, \"**\", \"*\"), recursive=True)\n",
        "             if os.path.splitext(p)[1] in IMG_EXTS]\n",
        "all_files = sorted(all_files, key=_natural_key)\n",
        "assert len(all_files) >= 500, f\"Found {len(all_files)} files, expected ~500.\"\n",
        "\n",
        "train_files = all_files[:400]\n",
        "pred_files  = all_files[400:500]  # last 100\n",
        "print(f\"Found {len(all_files)} images -> train {len(train_files)}, predict {len(pred_files)}\")\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "# Image decode (PNG/JPG via TF, TIFF via PIL fallback)\n",
        "\n",
        "def _decode_any_image(path):\n",
        "    path = tf.convert_to_tensor(path)\n",
        "    ext = tf.strings.lower(tf.strings.regex_replace(path, r'^.*\\.', '.'))\n",
        "    img_bin = tf.io.read_file(path)\n",
        "\n",
        "    def decode_png_jpg():\n",
        "        img = tf.io.decode_image(img_bin, channels=0, expand_animations=False)\n",
        "        return img\n",
        "\n",
        "    def decode_tiff_py(p):\n",
        "        import numpy as np\n",
        "        from PIL import Image as _PILImage\n",
        "        arr = np.array(_PILImage.open(p.decode('utf-8')))\n",
        "        if arr.ndim == 2:\n",
        "            arr = arr[..., None]\n",
        "        return arr\n",
        "\n",
        "    is_tiff = tf.reduce_any([tf.equal(ext, s) for s in [\".tif\", \".tiff\"]])\n",
        "    img = tf.cond(\n",
        "        is_tiff,\n",
        "        lambda: tf.numpy_function(decode_tiff_py, [path], Tout=tf.uint8),\n",
        "        lambda: decode_png_jpg()\n",
        "    )\n",
        "    img.set_shape([None, None, None])  # H W C\n",
        "    c = tf.shape(img)[-1]\n",
        "    img = tf.cond(tf.equal(c, 1), lambda: img, lambda: tf.image.rgb_to_grayscale(img))\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)  # [0,1]\n",
        "    img = img - 0.5                                  # [-0.5,0.5]\n",
        "    return img  # [H,W,1]\n",
        "\n",
        "\n",
        "def _pad_to_multiple_tf(img, mult=TILE):\n",
        "    h = tf.shape(img)[0]; w = tf.shape(img)[1]\n",
        "    pad_h = (mult - (h % mult)) % mult\n",
        "    pad_w = (mult - (w % mult)) % mult\n",
        "    img = tf.pad(img, [[0, pad_h], [0, pad_w], [0, 0]], mode='SYMMETRIC')\n",
        "    return img\n",
        "\n",
        "\n",
        "def _tile_1024_tf(img):\n",
        "    img = _pad_to_multiple_tf(img, TILE)\n",
        "    ks = [1, TILE, TILE, 1]\n",
        "    st = [1, TILE, TILE, 1]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images=tf.expand_dims(img, 0),\n",
        "        sizes=ks, strides=st, rates=[1,1,1,1], padding='VALID')\n",
        "    n_h = tf.shape(patches)[1]\n",
        "    n_w = tf.shape(patches)[2]\n",
        "    patches = tf.reshape(patches, [n_h * n_w, TILE, TILE, 1])\n",
        "    return patches\n",
        "\n",
        "\n",
        "def files_to_tiles_ds(files, batch, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(files), reshuffle_each_iteration=True)\n",
        "\n",
        "    def map_decode(path):\n",
        "        img = _decode_any_image(path)\n",
        "        tiles = _tile_1024_tf(img)\n",
        "        tile_ds = tf.data.Dataset.from_tensor_slices(tiles)\n",
        "        return tile_ds\n",
        "\n",
        "    ds = ds.interleave(map_decode, cycle_length=8, num_parallel_calls=AUTO, deterministic=False)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(2048)\n",
        "    ds = ds.map(lambda x: (x, x), num_parallel_calls=AUTO)  # autoencoder target\n",
        "    ds = ds.batch(batch).prefetch(AUTO)\n",
        "    return ds\n",
        "\n",
        "train_ds = files_to_tiles_ds(train_files, batch=BATCH_SIZE, shuffle=True)\n",
        "val_ds   = files_to_tiles_ds(pred_files,  batch=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "for xb, yb in val_ds.take(1):\n",
        "    print(\"Sample batch:\", xb.shape, yb.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# (G) Callbacks & Export helpers\n",
        "# -----------------------------\n",
        "\n",
        "def make_top_callbacks(out_dir,\n",
        "                       monitor_primary=\"val_ssim_metric\",\n",
        "                       monitor_secondary=\"val_loss\"):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    return [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(out_dir, \"top_best_by_ssim.weights.h5\"),\n",
        "            save_weights_only=True, save_best_only=True,\n",
        "            monitor=monitor_primary, mode=\"max\", verbose=1),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(out_dir, \"top_best_by_vloss.weights.h5\"),\n",
        "            save_weights_only=True, save_best_only=True,\n",
        "            monitor=monitor_secondary, mode=\"min\", verbose=1),\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(out_dir, \"top_log.csv\")),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=os.path.join(out_dir, \"tb_top\")),\n",
        "    ]\n",
        "\n",
        "\n",
        "def _to_uint8(x):\n",
        "    x = np.clip(x, 0.0, 1.0)\n",
        "    return (x * 255.0 + 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "def save_png01(img01, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    Image.fromarray(_to_uint8(img01)).save(path, 'PNG')\n",
        "\n",
        "\n",
        "def _top_manifest_dict():\n",
        "    return {\n",
        "        \"IMAGE_SIZE\": IMAGE_SIZE,\n",
        "        \"TOP_GRID\": TOP_GRID,\n",
        "        \"NUM_EMBEDDINGS_TOP\": NUM_EMBEDDINGS_TOP,\n",
        "        \"EMBEDDING_DIM_TOP\": EMBEDDING_DIM_TOP,\n",
        "        \"COMMITMENT_COST_TOP\": COMMITMENT_COST_TOP,\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "\n",
        "\n",
        "def export_top_bundle(model, out_root=\"/content/drive/MyDrive/vqvae_toponly_runs\",\n",
        "                      run_dir=None, history=None, take_val=1, also_savedmodel=False):\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    stamp = time.strftime(\"%Y%m%d-%H%M%S\") if run_dir is None else run_dir\n",
        "    bundle_dir = os.path.join(out_root, stamp)\n",
        "    os.makedirs(bundle_dir, exist_ok=True)\n",
        "\n",
        "    # build once\n",
        "    _ = model(tf.zeros([1, IMAGE_SIZE, IMAGE_SIZE, 1]), training=False)\n",
        "\n",
        "    # 1) weights\n",
        "    w_path = os.path.join(bundle_dir, \"toponly.weights.h5\")\n",
        "    model.save_weights(w_path)\n",
        "\n",
        "    # 2) codebook + EMA\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_emb.npy\"), model.vq_top.embeddings.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_ema_cluster_size.npy\"), model.vq_top.ema_cluster_size.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_ema_dw.npy\"), model.vq_top.ema_dw.numpy())\n",
        "\n",
        "    # 3) manifest\n",
        "    with open(os.path.join(bundle_dir, \"manifest.json\"), \"w\") as f:\n",
        "        json.dump(_top_manifest_dict(), f, indent=2)\n",
        "\n",
        "    # 4) history\n",
        "    if history is not None and hasattr(history, \"history\"):\n",
        "        with open(os.path.join(bundle_dir, \"history.json\"), \"w\") as f:\n",
        "            json.dump(history.history, f, indent=2)\n",
        "\n",
        "    # 5) sample from val_ds\n",
        "    sample_dir = os.path.join(bundle_dir, \"samples\"); os.makedirs(sample_dir, exist_ok=True)\n",
        "    taken = 0\n",
        "    for xb, _ in val_ds.take(take_val):\n",
        "        yb = model(xb, training=False)\n",
        "        save_png01((xb.numpy()[0,...,0] + 0.5), os.path.join(sample_dir, f\"ae_input_val0.png\"))\n",
        "        save_png01((yb.numpy()[0,...,0] + 0.5), os.path.join(sample_dir, f\"ae_recon_val0.png\"))\n",
        "        taken += 1\n",
        "        if taken >= take_val: break\n",
        "\n",
        "    # 6) SavedModel\n",
        "    if also_savedmodel:\n",
        "        sm_dir = os.path.join(bundle_dir, \"saved_model\")\n",
        "        model.save(sm_dir)\n",
        "\n",
        "    # 7) latest marker\n",
        "    latest = os.path.join(out_root, \"latest\")\n",
        "    try:\n",
        "        if os.path.islink(latest) or os.path.exists(latest):\n",
        "            os.remove(latest)\n",
        "        os.symlink(bundle_dir, latest)\n",
        "    except Exception:\n",
        "        with open(os.path.join(out_root, \"LATEST.txt\"), \"w\") as f:\n",
        "            f.write(bundle_dir)\n",
        "\n",
        "    print(f\"✅ Top-only bundle saved to: {bundle_dir}\")\n",
        "    return bundle_dir\n",
        "\n",
        "# -----------------------------\n",
        "# (H) Bit-accurate compression math (Top-only)\n",
        "# -----------------------------\n",
        "bits_per_top  = int(math.ceil(math.log2(NUM_EMBEDDINGS_TOP)))   # 256 -> 8\n",
        "tokens_top    = TOP_GRID * TOP_GRID                             # 1024\n",
        "latent_bits   = tokens_top * bits_per_top                       # 8192\n",
        "orig_bits     = IMAGE_SIZE * IMAGE_SIZE * 8                     # 8,388,608 (for 1024x1024, 8-bit)\n",
        "bpp           = latent_bits / (IMAGE_SIZE * IMAGE_SIZE)         # ≈0.0078125\n",
        "ratio         = orig_bits / latent_bits                         # ≈1024×\n",
        "\n",
        "print(\"=\"*66)\n",
        "print(f\"Bit-accurate compression (Top-only): {ratio:.1f}×  (~{bpp:.5f} bpp)\")\n",
        "print(f\" - tokens: top {tokens_top}\")\n",
        "print(f\" - bits/token: top {bits_per_top}\")\n",
        "print(f\" - commitment(top) = {COMMITMENT_COST_TOP}\")\n",
        "print(\"=\"*66)\n",
        "\n",
        "# -----------------------------\n",
        "# (I) Train → load best → export\n",
        "# -----------------------------\n",
        "run_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "out_root = \"/content/drive/MyDrive/vqvae_toponly_runs\"\n",
        "run_dir   = os.path.join(out_root, run_stamp)\n",
        "\n",
        "cbs = make_top_callbacks(\n",
        "    run_dir,\n",
        "    monitor_primary=\"val_ssim_metric\",\n",
        "    monitor_secondary=\"val_loss\",\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_WARMUP,\n",
        "    callbacks=cbs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "best_ckpt = os.path.join(run_dir, \"top_best_by_vloss.weights.h5\")\n",
        "model.load_weights(best_ckpt)\n",
        "print(f\"Loaded best weights: {best_ckpt}\")\n",
        "\n",
        "bundle_dir = export_top_bundle(\n",
        "    model,\n",
        "    out_root=out_root,\n",
        "    run_dir=run_stamp,\n",
        "    history=history,\n",
        "    also_savedmodel=False\n",
        ")\n",
        "print(\"Exported:\", bundle_dir)\n",
        "\n",
        "# -----------------------------\n",
        "# (J) Predict last-100 full images -> stitch back to full size\n",
        "# -----------------------------\n",
        "\n",
        "def reconstruct_image(model, path, tile=None, batch=4, out_dir=\"/content/recons_full\"):\n",
        "    \"\"\"Reconstruct one full image by tiling with given tile size and stitching back.\n",
        "    Saves PNG as <basename>_recon.png, returns path.\n",
        "    \"\"\"\n",
        "    if tile is None:\n",
        "        tile = IMAGE_SIZE\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "    # Decode to numpy using the same preproc\n",
        "    img_t = _decode_any_image(tf.constant(path))   # [-0.5,0.5]\n",
        "    img = img_t.numpy()\n",
        "    h0, w0 = img.shape[:2]\n",
        "\n",
        "    # Pad symmetrically to multiple of tile\n",
        "    pad_h = (tile - (h0 % tile)) % tile\n",
        "    pad_w = (tile - (w0 % tile)) % tile\n",
        "    img_pad = np.pad(img, ((0, pad_h), (0, pad_w), (0, 0)), mode='symmetric')\n",
        "    h, w = img_pad.shape[:2]\n",
        "\n",
        "    n_h = h // tile\n",
        "    n_w = w // tile\n",
        "\n",
        "    # Batch through the model\n",
        "    recons_tiles = []\n",
        "    batch_buf = []\n",
        "    for ih in range(n_h):\n",
        "        for iw in range(n_w):\n",
        "            y = ih * tile; x = iw * tile\n",
        "            patch = img_pad[y:y+tile, x:x+tile, :]\n",
        "            batch_buf.append(patch)\n",
        "            if len(batch_buf) == batch:\n",
        "                xb = tf.convert_to_tensor(np.stack(batch_buf, axis=0), dtype=tf.float32)\n",
        "                yb = model(xb, training=False).numpy()\n",
        "                recons_tiles.append(yb)\n",
        "                batch_buf = []\n",
        "    if len(batch_buf) > 0:\n",
        "        xb = tf.convert_to_tensor(np.stack(batch_buf, axis=0), dtype=tf.float32)\n",
        "        yb = model(xb, training=False).numpy()\n",
        "        recons_tiles.append(yb)\n",
        "        batch_buf = []\n",
        "    recons = np.concatenate(recons_tiles, axis=0) if len(recons_tiles) else np.zeros((0, tile, tile, 1), np.float32)\n",
        "\n",
        "    # Stitch back\n",
        "    canvas = np.zeros((h, w, 1), dtype=np.float32)\n",
        "    k = 0\n",
        "    for ih in range(n_h):\n",
        "        for iw in range(n_w):\n",
        "            y = ih * tile; x = iw * tile\n",
        "            canvas[y:y+tile, x:x+tile, :] = recons[k]\n",
        "            k += 1\n",
        "\n",
        "    # Crop to original size and save\n",
        "    canvas = canvas[:h0, :w0, :]\n",
        "    out = (np.clip(canvas + 0.5, 0.0, 1.0) * 255.0 + 0.5).astype(np.uint8)\n",
        "    base = os.path.splitext(os.path.basename(path))[0]\n",
        "    save_path = os.path.join(out_dir, f\"{base}_recon.png\")\n",
        "    Image.fromarray(out[...,0]).save(save_path, \"PNG\")\n",
        "    return save_path\n",
        "\n",
        "if RUN_TRAIN:\n",
        "    # keep the original behavior: only last-100\n",
        "    files_to_recon = pred_files\n",
        "    full_out_dir = os.path.join(run_dir, \"recons_full\")\n",
        "    recon_desc = \"Predict & stitch last-100\"\n",
        "else:\n",
        "    # reconstruct ALL images using the provided checkpoint\n",
        "    files_to_recon = all_files\n",
        "    full_out_dir = os.path.join(run_dir, \"recons_full_all\")\n",
        "    recon_desc = f\"Reconstruct ALL ({len(all_files)}) from CKPT\"\n",
        "\n",
        "print(\"Reconstruction output dir:\", full_out_dir)\n",
        "os.makedirs(full_out_dir, exist_ok=True)\n",
        "saved = []\n",
        "for p in tqdm(files_to_recon, desc=recon_desc):\n",
        "    saved.append(reconstruct_image(model, p, tile=IMAGE_SIZE, batch=max(1, BATCH_SIZE), out_dir=full_out_dir))\n",
        "print(f\"Saved {len(saved)} reconstructions to {full_out_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7sSmad-Y6F-"
      },
      "outputs": [],
      "source": [
        "# =========================== Eval: Top-only 重组第一张测试图 ===========================\n",
        "import os, glob, json, time, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# model.load_weights(\"/content/drive/MyDrive/vqvae_toponly_runs/你的时间戳/top_best_by_ssim.weights.h5\")\n",
        "\n",
        "def _to_uint8(x):\n",
        "    x = np.clip(x, 0.0, 1.0)\n",
        "    return (x * 255.0 + 0.5).astype(np.uint8)\n",
        "\n",
        "def save_png01(img01, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    Image.fromarray(_to_uint8(img01)).save(path, 'PNG')\n",
        "\n",
        "def find_first_test_image(test_dir):\n",
        "    paths = sorted(glob.glob(os.path.join(test_dir, '*.png')))\n",
        "    if len(paths) == 0:\n",
        "        paths = sorted(glob.glob(os.path.join(test_dir, '**/*.png'), recursive=True))\n",
        "    if len(paths) == 0:\n",
        "        raise FileNotFoundError(f\"No PNG images found under {test_dir}\")\n",
        "    return paths[0]\n",
        "\n",
        "\n",
        "def load_img_for_model(path):\n",
        "    return load_image(path)\n",
        "\n",
        "def eval_recon_toponly(model, test_path,\n",
        "                       out_root=\"/content/drive/MyDrive/vqvae_toponly_eval\"):\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    out_dir = os.path.join(out_root, stamp)\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    x  = load_img_for_model(test_path)          # [H,W,1], [-0.5,0.5]\n",
        "    x1 = tf.expand_dims(x, 0)                   # [1,H,W,1]\n",
        "    y1 = model(x1, training=False)              # [1,H,W,1]\n",
        "\n",
        "\n",
        "    x01 = (x1.numpy()[0, ..., 0] + 0.5)\n",
        "    y01 = (y1.numpy()[0, ..., 0] + 0.5)\n",
        "\n",
        "\n",
        "    psnr = float(tf.image.psnr(x1 + 0.5, y1 + 0.5, max_val=1.0).numpy().mean())\n",
        "    ssim = float(tf.image.ssim(x1 + 0.5, y1 + 0.5, max_val=1.0).numpy().mean())\n",
        "\n",
        "\n",
        "    in_path = os.path.join(out_dir, \"input.png\")\n",
        "    re_path = os.path.join(out_dir, \"recon.png\")\n",
        "    save_png01(x01, in_path)\n",
        "    save_png01(y01, re_path)\n",
        "\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 4), dpi=150)\n",
        "    ax = plt.subplot(1, 2, 1); ax.imshow(x01, cmap='gray'); ax.set_title(\"Input\"); ax.axis('off')\n",
        "    ax = plt.subplot(1, 2, 2); ax.imshow(y01, cmap='gray'); ax.set_title(\"Recon\"); ax.axis('off')\n",
        "    panel_path = os.path.join(out_dir, \"panel.png\")\n",
        "    plt.tight_layout(); plt.savefig(panel_path, dpi=150); plt.close(fig)\n",
        "\n",
        "\n",
        "    bits_per_top = int(math.ceil(math.log2(int(NUM_EMBEDDINGS_TOP))))\n",
        "    tokens_top   = int(TOP_GRID) * int(TOP_GRID)\n",
        "    latent_bits  = tokens_top * bits_per_top\n",
        "    orig_bits    = int(IMAGE_SIZE) * int(IMAGE_SIZE) * 8\n",
        "    ratio        = orig_bits / latent_bits\n",
        "    bpp          = latent_bits / (IMAGE_SIZE * IMAGE_SIZE)\n",
        "\n",
        "    info = {\n",
        "        \"test_path\": test_path,\n",
        "        \"psnr\": round(psnr, 4),\n",
        "        \"ssim\": round(ssim, 6),\n",
        "        \"tokens_top\": tokens_top,\n",
        "        \"bits_per_token\": bits_per_top,\n",
        "        \"latent_bits\": latent_bits,\n",
        "        \"orig_bits\": orig_bits,\n",
        "        \"compression_ratio\": round(ratio, 2),\n",
        "        \"bpp\": bpp\n",
        "    }\n",
        "    with open(os.path.join(out_dir, \"results.json\"), \"w\") as f:\n",
        "        json.dump(info, f, indent=2)\n",
        "\n",
        "    print(f\"[Test] {test_path}\")\n",
        "    print(f\"[Metrics] PSNR={psnr:.2f}, SSIM={ssim:.4f}\")\n",
        "    print(f\"[Bits] tokens={tokens_top}, bits/token={bits_per_top}, \"\n",
        "          f\"latent={latent_bits}, orig={orig_bits}, ratio≈{ratio:.1f}× (~{bpp:.5f} bpp)\")\n",
        "    print(\"✅ Saved:\", in_path)\n",
        "    print(\"✅ Saved:\", re_path)\n",
        "    print(\"✅ Saved:\", panel_path)\n",
        "    print(\"✅ Saved:\", os.path.join(out_dir, \"results.json\"))\n",
        "    return out_dir\n",
        "\n",
        "\n",
        "test_img_path = find_first_test_image(TEST_DIR)\n",
        "_ = eval_recon_toponly(model, test_img_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSGxSiyqrYZ3"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "def get_test_image_paths(test_dir, n=30):\n",
        "    paths = sorted(glob.glob(os.path.join(test_dir, '*.png')))\n",
        "    if len(paths) == 0:\n",
        "        paths = sorted(glob.glob(os.path.join(test_dir, '**/*.png'), recursive=True))\n",
        "    if len(paths) == 0:\n",
        "        raise FileNotFoundError(f\"No PNG images found under {test_dir}\")\n",
        "    return paths[:n]\n",
        "\n",
        "def eval_recon_toponly_many(model, test_paths, out_root=\"/content/drive/MyDrive/vqvae_toponly_eval\", batch_size=2):\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    out_dir = os.path.join(out_root, f\"{stamp}_batch\")\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    bits_per_top = int(math.ceil(math.log2(int(NUM_EMBEDDINGS_TOP))))\n",
        "    tokens_top   = int(TOP_GRID) * int(TOP_GRID)\n",
        "    latent_bits  = tokens_top * bits_per_top\n",
        "    orig_bits    = int(IMAGE_SIZE) * int(IMAGE_SIZE) * 8\n",
        "    ratio        = orig_bits / latent_bits\n",
        "    bpp          = latent_bits / (IMAGE_SIZE * IMAGE_SIZE)\n",
        "\n",
        "    results = []\n",
        "    for i in range(0, len(test_paths), batch_size):\n",
        "        chunk = test_paths[i:i+batch_size]\n",
        "        xs = [load_img_for_model(p) for p in chunk]\n",
        "        x1 = tf.stack(xs, axis=0)                     # [B,H,W,1]\n",
        "        y1 = model(x1, training=False)\n",
        "\n",
        "\n",
        "        psnrs = tf.image.psnr(x1 + 0.5, y1 + 0.5, max_val=1.0).numpy()\n",
        "        ssims = tf.image.ssim(x1 + 0.5, y1 + 0.5, max_val=1.0).numpy()\n",
        "\n",
        "        x01 = (x1.numpy() + 0.5)[..., 0]  # [B,H,W] in [0,1]\n",
        "        y01 = (y1.numpy() + 0.5)[..., 0]\n",
        "\n",
        "        for j, p in enumerate(chunk):\n",
        "            idx = i + j\n",
        "            stem = f\"{idx:04d}\"\n",
        "            save_png01(x01[j], os.path.join(out_dir, f\"{stem}_input.png\"))\n",
        "            save_png01(y01[j], os.path.join(out_dir, f\"{stem}_recon.png\"))\n",
        "            results.append({\n",
        "                \"index\": idx,\n",
        "                \"test_path\": p,\n",
        "                \"psnr\": float(psnrs[j]),\n",
        "                \"ssim\": float(ssims[j])\n",
        "            })\n",
        "\n",
        "\n",
        "    avg_psnr = float(np.mean([r[\"psnr\"] for r in results]))\n",
        "    avg_ssim = float(np.mean([r[\"ssim\"] for r in results]))\n",
        "\n",
        "    summary = {\n",
        "        \"count\": len(test_paths),\n",
        "        \"avg_psnr\": round(avg_psnr, 4),\n",
        "        \"avg_ssim\": round(avg_ssim, 6),\n",
        "        \"tokens_top\": tokens_top,\n",
        "        \"bits_per_token\": bits_per_top,\n",
        "        \"latent_bits\": latent_bits,\n",
        "        \"orig_bits\": orig_bits,\n",
        "        \"compression_ratio\": round(ratio, 2),\n",
        "        \"bpp\": bpp\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(out_dir, \"results.json\"), \"w\") as f:\n",
        "        json.dump({\"summary\": summary, \"results\": results}, f, indent=2)\n",
        "\n",
        "    with open(os.path.join(out_dir, \"results.csv\"), \"w\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"index\", \"test_path\", \"psnr\", \"ssim\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(results)\n",
        "\n",
        "    print(f\"[BATCH DONE] N={len(test_paths)} | Avg PSNR={avg_psnr:.2f}, Avg SSIM={avg_ssim:.4f}\")\n",
        "    print(f\"Saved to: {out_dir}\")\n",
        "    return out_dir\n",
        "\n",
        "# TEST_DIR = \"/content/drive/MyDrive/your_test_dir\"\n",
        "paths_30 = get_test_image_paths(TEST_DIR, n=30)\n",
        "_ = eval_recon_toponly_many(model, paths_30, batch_size=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leJJoKpUSXJE"
      },
      "source": [
        "## 2) Two-Level VQ-VAE (204×)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVyRUzygxmHS"
      },
      "outputs": [],
      "source": [
        "# ==============================================================\n",
        "# Stage-1 (Two-level VQ-VAE-2, EMA codebook + LN/Dropout + FiLM)\n",
        "# - Top = 32x32, Bottom = 64x64\n",
        "# - Bottom commitment = 0.15\n",
        "# ==============================================================\n",
        "\n",
        "import os, time, json, math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# -----------------------------\n",
        "# (A) Residual helpers\n",
        "# -----------------------------\n",
        "if 'residual_stack' not in globals():\n",
        "    def residual_block(x, filters):\n",
        "        skip = x\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        x = tf.keras.layers.Conv2D(filters//2, 3, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "        x = tf.keras.layers.Conv2D(filters, 1, padding='same')(x)\n",
        "        return tf.keras.layers.Add()([skip, x])\n",
        "\n",
        "    def residual_stack(x, filters, num_blocks=2):\n",
        "        for _ in range(num_blocks):\n",
        "            x = residual_block(x, filters)\n",
        "        return tf.keras.layers.ReLU()(x)\n",
        "\n",
        "# -----------------------------\n",
        "# (B) EMA VectorQuantizer (VQ-VAE-2)\n",
        "# -----------------------------\n",
        "class VectorQuantizerEMA(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_embeddings, embedding_dim,\n",
        "                 commitment_cost=0.25, decay=0.99, epsilon=1e-5, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_embeddings = int(num_embeddings)\n",
        "        self.embedding_dim = int(embedding_dim)\n",
        "        self.commitment_cost = float(commitment_cost)\n",
        "        self.decay = float(decay)\n",
        "        self.epsilon = float(epsilon)\n",
        "        self._perplexity = tf.keras.metrics.Mean(name=f'{self.name}_perplexity')\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self._perplexity]\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.embeddings = self.add_weight(\n",
        "            name='embeddings',\n",
        "            shape=(self.embedding_dim, self.num_embeddings),\n",
        "            initializer=tf.keras.initializers.RandomUniform(-1.0/self.num_embeddings, 1.0/self.num_embeddings),\n",
        "            trainable=False\n",
        "        )\n",
        "        self.ema_cluster_size = self.add_weight(\n",
        "            name='ema_cluster_size',\n",
        "            shape=(self.num_embeddings,),\n",
        "            initializer='zeros',\n",
        "            trainable=False\n",
        "        )\n",
        "        self.ema_dw = self.add_weight(\n",
        "            name='ema_dw',\n",
        "            shape=(self.embedding_dim, self.num_embeddings),\n",
        "            initializer='zeros',\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        shp  = tf.shape(inputs)                              # [B,H,W,C]\n",
        "        flat = tf.reshape(inputs, [-1, self.embedding_dim])  # [N,C]\n",
        "\n",
        "        # NN lookup via expanded L2\n",
        "        dists = (tf.reduce_sum(flat**2, axis=1, keepdims=True)\n",
        "                 - 2.0 * tf.matmul(flat, self.embeddings)\n",
        "                 + tf.reduce_sum(self.embeddings**2, axis=0, keepdims=True))  # [N,K]\n",
        "        idx  = tf.argmax(-dists, axis=1)                     # [N]\n",
        "        one  = tf.one_hot(idx, self.num_embeddings, dtype=flat.dtype)  # [N,K]\n",
        "        quant = tf.matmul(one, tf.transpose(self.embeddings))          # [N,C]\n",
        "        quant = tf.reshape(quant, shp)                                   # [B,H,W,C]\n",
        "\n",
        "        # EMA updates (training only)\n",
        "        def _ema_update():\n",
        "            cluster_size = tf.reduce_sum(one, axis=0)                  # [K]\n",
        "            dw = tf.matmul(tf.transpose(flat), one)                    # [C,K]\n",
        "            ema_cs = self.ema_cluster_size * self.decay + cluster_size * (1.0 - self.decay)\n",
        "            ema_dw = self.ema_dw * self.decay + dw * (1.0 - self.decay)\n",
        "            n = tf.reduce_sum(ema_cs)\n",
        "            smoothed_cs = (ema_cs + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n\n",
        "            new_embed = ema_dw / tf.expand_dims(smoothed_cs, 0)        # [C,K]\n",
        "            self.ema_cluster_size.assign(ema_cs)\n",
        "            self.ema_dw.assign(ema_dw)\n",
        "            self.embeddings.assign(new_embed)\n",
        "            return 0.0\n",
        "\n",
        "        if training is None:\n",
        "            training = tf.keras.backend.learning_phase()\n",
        "        tf.cond(tf.cast(training, tf.bool), _ema_update, lambda: 0.0)\n",
        "\n",
        "        # only commitment loss\n",
        "        e_loss = tf.reduce_mean((tf.stop_gradient(quant) - inputs)**2)\n",
        "        self.add_loss(self.commitment_cost * e_loss)\n",
        "\n",
        "        # perplexity\n",
        "        avg_p = tf.reduce_mean(one, axis=0)\n",
        "        perp  = tf.exp(-tf.reduce_sum(avg_p * tf.math.log(avg_p + 1e-10)))\n",
        "        self._perplexity.update_state(perp)\n",
        "\n",
        "        # straight-through\n",
        "        quant = inputs + tf.stop_gradient(quant - inputs)\n",
        "        return quant\n",
        "\n",
        "# -----------------------------\n",
        "# (C) Hyperparams (read/override from globals when present)\n",
        "# -----------------------------\n",
        "IMAGE_SIZE         = int(globals().get('IMAGE_SIZE', 1024))\n",
        "TOP_GRID           = int(globals().get('TOP_GRID', 32))\n",
        "BOTTOM_GRID        = int(globals().get('BOTTOM_GRID', 64))\n",
        "NUM_HIDDENS_TOP    = int(globals().get('NUM_HIDDENS_TOP', 128))\n",
        "NUM_HIDDENS_BOTTOM = int(globals().get('NUM_HIDDENS_BOTTOM', 128))\n",
        "\n",
        "EMBEDDING_DIM_TOP     = int(globals().get('EMBEDDING_DIM_TOP', 96))\n",
        "EMBEDDING_DIM_BOTTOM  = int(globals().get('EMBEDDING_DIM_BOTTOM', 96))\n",
        "NUM_EMBEDDINGS_TOP    = int(globals().get('NUM_EMBEDDINGS_TOP', 256))\n",
        "NUM_EMBEDDINGS_BOTTOM = int(globals().get('NUM_EMBEDDINGS_BOTTOM', 256))\n",
        "\n",
        "COMMITMENT_COST_TOP    = float(globals().get('COMMITMENT_COST_TOP', 1.0))\n",
        "COMMITMENT_COST_BOTTOM = float(globals().get('COMMITMENT_COST_BOTTOM', 0.15))\n",
        "# --- Hyperparams ---\n",
        "COMMITMENT_COST_TOP    = float(globals().get('COMMITMENT_COST_TOP', 1.0))\n",
        "COMMITMENT_COST_BOTTOM = 0.15\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# (D) Building blocks\n",
        "# -----------------------------\n",
        "def build_encoder_bottom(img_size, bottom_grid, num_hiddens):\n",
        "    assert (img_size % bottom_grid) == 0\n",
        "    n_downs = int(math.log2(img_size // bottom_grid))  # 1024->64 = 4\n",
        "    inputs = tf.keras.Input(shape=(img_size, img_size, 1))\n",
        "    x = inputs\n",
        "    chans = [64, 128, 128, num_hiddens][:n_downs]\n",
        "    for c in chans:\n",
        "        x = tf.keras.layers.Conv2D(c, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    x = residual_stack(x, num_hiddens, num_blocks=2)\n",
        "    return tf.keras.Model(inputs, x, name=f'encoder_bottom_{bottom_grid}')\n",
        "\n",
        "def build_encoder_top_from_bottom(bottom_grid, top_grid, in_ch, out_ch):\n",
        "    assert (bottom_grid % top_grid) == 0\n",
        "    n_downs = int(math.log2(bottom_grid // top_grid))  # 64->32 = 1\n",
        "    inputs = tf.keras.Input(shape=(bottom_grid, bottom_grid, in_ch))\n",
        "    x = inputs\n",
        "    for _ in range(n_downs):\n",
        "        x = tf.keras.layers.Conv2D(out_ch, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    x = residual_stack(x, out_ch, num_blocks=2)\n",
        "    return tf.keras.Model(inputs, x, name=f'encoder_top_{top_grid}')\n",
        "\n",
        "def build_top_to_bottom_upsampler(top_grid, bottom_grid, in_ch, out_ch):\n",
        "    assert (bottom_grid % top_grid) == 0\n",
        "    n_ups = int(math.log2(bottom_grid // top_grid))     # 32->64 = 1\n",
        "    inputs = tf.keras.Input(shape=(top_grid, top_grid, in_ch))\n",
        "    x = inputs\n",
        "    for _ in range(n_ups):\n",
        "        x = tf.keras.layers.Conv2DTranspose(out_ch, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    return tf.keras.Model(inputs, x, name=f'top_upsampler_{top_grid}_to_{bottom_grid}')\n",
        "\n",
        "def build_decoder_bottom_to_image(img_size, bottom_grid, in_ch):\n",
        "    assert (img_size % bottom_grid) == 0\n",
        "    n_ups = int(math.log2(img_size // bottom_grid))    # 64->1024 = 4\n",
        "    inputs = tf.keras.Input(shape=(bottom_grid, bottom_grid, in_ch))\n",
        "    x = inputs\n",
        "    ups_ch = [256, 128, 128, 64][:n_ups]\n",
        "    for c in ups_ch:\n",
        "        x = tf.keras.layers.Conv2DTranspose(c, 4, strides=2, padding='same')(x)\n",
        "        x = tf.keras.layers.ReLU()(x)\n",
        "    out = tf.keras.layers.Conv2D(1, 3, padding='same')(x)\n",
        "    return tf.keras.Model(inputs, out, name=f'decoder_bottom_{bottom_grid}_to_{IMAGE_SIZE}')\n",
        "\n",
        "# -----------------------------\n",
        "# (E) Assemble Stage-1 model\n",
        "# -----------------------------\n",
        "enc_bottom = build_encoder_bottom(IMAGE_SIZE, BOTTOM_GRID, NUM_HIDDENS_BOTTOM)\n",
        "enc_top    = build_encoder_top_from_bottom(BOTTOM_GRID, TOP_GRID, NUM_HIDDENS_BOTTOM, NUM_HIDDENS_TOP)\n",
        "\n",
        "pre_vq_top     = tf.keras.layers.Conv2D(EMBEDDING_DIM_TOP, 1, name='pre_vq_top')\n",
        "vq_top         = VectorQuantizerEMA(NUM_EMBEDDINGS_TOP, EMBEDDING_DIM_TOP,\n",
        "                                    commitment_cost=COMMITMENT_COST_TOP, decay=0.99, epsilon=1e-5, name='vq_top')\n",
        "post_vq_top    = tf.keras.layers.Conv2D(NUM_HIDDENS_TOP, 1, name='post_vq_top')\n",
        "\n",
        "pre_vq_bottom  = tf.keras.layers.Conv2D(EMBEDDING_DIM_BOTTOM, 1, name='pre_vq_bottom')\n",
        "vq_bottom      = VectorQuantizerEMA(NUM_EMBEDDINGS_BOTTOM, EMBEDDING_DIM_BOTTOM,\n",
        "                                    commitment_cost=COMMITMENT_COST_BOTTOM, decay=0.99, epsilon=1e-5, name='vq_bottom')\n",
        "post_vq_bottom = tf.keras.layers.Conv2D(NUM_HIDDENS_BOTTOM, 1, name='post_vq_bottom')\n",
        "\n",
        "top_up   = build_top_to_bottom_upsampler(TOP_GRID, BOTTOM_GRID, NUM_HIDDENS_TOP, NUM_HIDDENS_BOTTOM)\n",
        "dec_bot  = build_decoder_bottom_to_image(IMAGE_SIZE, BOTTOM_GRID, in_ch=NUM_HIDDENS_BOTTOM * 2)\n",
        "\n",
        "class VQTwoLevelEMA(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.enc_bottom = enc_bottom\n",
        "        self.enc_top    = enc_top\n",
        "        self.pre_vq_top = pre_vq_top\n",
        "        self.vq_top     = vq_top\n",
        "        self.post_vq_top= post_vq_top\n",
        "        self.pre_vq_bottom  = pre_vq_bottom\n",
        "        self.vq_bottom      = vq_bottom\n",
        "        self.post_vq_bottom = post_vq_bottom\n",
        "        self.top_up   = top_up\n",
        "        self.dec_bottom = dec_bot\n",
        "        # info balancing\n",
        "        self.norm_b = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "        self.norm_t = tf.keras.layers.LayerNormalization(axis=-1)\n",
        "        self.drop_b = tf.keras.layers.SpatialDropout2D(0.3)  # training only\n",
        "        # FiLM\n",
        "        self.film_gamma = tf.keras.layers.Conv2D(NUM_HIDDENS_BOTTOM, 1)\n",
        "        self.film_beta  = tf.keras.layers.Conv2D(NUM_HIDDENS_BOTTOM, 1)\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        hb  = self.enc_bottom(x)                 # [B,64,64,Hb]\n",
        "        zb  = self.pre_vq_bottom(hb)             # [B,64,64,Cb]\n",
        "        zbq = self.vq_bottom(zb, training=training)\n",
        "        zbq = self.post_vq_bottom(zbq)           # [B,64,64,Hb]\n",
        "\n",
        "        ht  = self.enc_top(hb)                   # [B,32,32,Ht]\n",
        "        zt  = self.pre_vq_top(ht)                # [B,32,32,Ct]\n",
        "        ztq = self.vq_top(zt, training=training)\n",
        "        ztq = self.post_vq_top(ztq)              # [B,32,32,Ht]\n",
        "        t_up= self.top_up(ztq)                   # [B,64,64,Hb]\n",
        "\n",
        "        zbq = self.norm_b(zbq)\n",
        "        t_up= self.norm_t(t_up)\n",
        "        zbq = self.drop_b(zbq, training=training)\n",
        "\n",
        "        gamma = self.film_gamma(t_up)\n",
        "        beta  = self.film_beta(t_up)\n",
        "        zbq_mod = zbq * (1.0 + gamma) + beta\n",
        "\n",
        "        y = self.dec_bottom(tf.concat([zbq_mod, t_up], axis=-1))  # [B,1024,1024,1]\n",
        "        return y\n",
        "\n",
        "model = VQTwoLevelEMA()\n",
        "\n",
        "# -----------------------------\n",
        "# (F) Compile (define if missing)\n",
        "# -----------------------------\n",
        "def ssim_metric(y_true, y_pred):\n",
        "    # data is in [-0.5, 0.5]; shift to [0,1] for SSIM\n",
        "    return tf.reduce_mean(tf.image.ssim(y_true + 0.5, y_pred + 0.5, max_val=1.0))\n",
        "\n",
        "def psnr_metric(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.image.psnr(y_true + 0.5, y_pred + 0.5, max_val=1.0))\n",
        "\n",
        "def compile_warmup(m, lr=3e-4, wd=1e-4):\n",
        "    try:\n",
        "        opt = tf.keras.optimizers.AdamW(learning_rate=lr, weight_decay=wd, beta_1=0.9, beta_2=0.95)\n",
        "    except Exception:\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.95)\n",
        "    # L1 更稳，EMA VQ 的 loss 已由 layer.add_loss 注入\n",
        "    m.compile(optimizer=opt, loss='mae', metrics=[ssim_metric, psnr_metric])\n",
        "\n",
        "# -----------------------------\n",
        "# (G) Callbacks: auto-save best\n",
        "# -----------------------------\n",
        "def make_stage1_callbacks(out_dir,\n",
        "                          monitor_primary=\"val_ssim_metric\",\n",
        "                          monitor_secondary=\"val_loss\",\n",
        "                          patience=8):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    cbs = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(out_dir, \"stage1_best_by_ssim.weights.h5\"),\n",
        "            save_weights_only=True, save_best_only=True,\n",
        "            monitor=monitor_primary, mode=\"max\", verbose=1),\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            os.path.join(out_dir, \"stage1_best_by_vloss.weights.h5\"),\n",
        "            save_weights_only=True, save_best_only=True,\n",
        "            monitor=monitor_secondary, mode=\"min\", verbose=1),\n",
        "        tf.keras.callbacks.CSVLogger(os.path.join(out_dir, \"stage1_log.csv\")),\n",
        "        tf.keras.callbacks.TensorBoard(log_dir=os.path.join(out_dir, \"tb_stage1\")),\n",
        "        tf.keras.callbacks.EarlyStopping(monitor=monitor_secondary, mode=\"min\",\n",
        "                                         patience=patience, restore_best_weights=False),\n",
        "    ]\n",
        "    return cbs\n",
        "\n",
        "# -----------------------------\n",
        "# (H) Export bundle (weights + codebooks + manifest + samples)\n",
        "# -----------------------------\n",
        "def _to_uint8(x):\n",
        "    x = np.clip(x, 0.0, 1.0)\n",
        "    return (x * 255.0 + 0.5).astype(np.uint8)\n",
        "\n",
        "def save_png01(img01, path):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    Image.fromarray(_to_uint8(img01)).save(path, 'PNG')\n",
        "\n",
        "def _stage1_manifest_dict():\n",
        "    return {\n",
        "        \"IMAGE_SIZE\": IMAGE_SIZE,\n",
        "        \"TOP_GRID\": TOP_GRID,\n",
        "        \"BOTTOM_GRID\": BOTTOM_GRID,\n",
        "        \"NUM_EMBEDDINGS_TOP\": NUM_EMBEDDINGS_TOP,\n",
        "        \"NUM_EMBEDDINGS_BOTTOM\": NUM_EMBEDDINGS_BOTTOM,\n",
        "        \"EMBEDDING_DIM_TOP\": EMBEDDING_DIM_TOP,\n",
        "        \"EMBEDDING_DIM_BOTTOM\": EMBEDDING_DIM_BOTTOM,\n",
        "        \"COMMITMENT_COST_TOP\": COMMITMENT_COST_TOP,\n",
        "        \"COMMITMENT_COST_BOTTOM\": COMMITMENT_COST_BOTTOM,\n",
        "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    }\n",
        "\n",
        "def export_stage1_bundle(model, out_root=\"/content/drive/MyDrive/vqvae_stage1_runs\",\n",
        "                         run_dir=None, history=None, take_val=1, also_savedmodel=False):\n",
        "    os.makedirs(out_root, exist_ok=True)\n",
        "    stamp = time.strftime(\"%Y%m%d-%H%M%S\") if run_dir is None else run_dir\n",
        "    bundle_dir = os.path.join(out_root, stamp)\n",
        "    os.makedirs(bundle_dir, exist_ok=True)\n",
        "\n",
        "    # ensure built\n",
        "    try:\n",
        "        _xb = next(iter(train_ds))[0]\n",
        "        _ = model(_xb[:1], training=False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 1) weights\n",
        "    w_path = os.path.join(bundle_dir, \"stage1.weights.h5\")\n",
        "    model.save_weights(w_path)\n",
        "\n",
        "    # 2) codebooks + EMA\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_emb.npy\"),     model.vq_top.embeddings.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_bottom_emb.npy\"),  model.vq_bottom.embeddings.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_ema_cluster_size.npy\"),    model.vq_top.ema_cluster_size.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_top_ema_dw.npy\"),             model.vq_top.ema_dw.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_bottom_ema_cluster_size.npy\"), model.vq_bottom.ema_cluster_size.numpy())\n",
        "    np.save(os.path.join(bundle_dir, \"vq_bottom_ema_dw.npy\"),          model.vq_bottom.ema_dw.numpy())\n",
        "\n",
        "    # 3) manifest\n",
        "    with open(os.path.join(bundle_dir, \"manifest.json\"), \"w\") as f:\n",
        "        json.dump(_stage1_manifest_dict(), f, indent=2)\n",
        "\n",
        "    # 4) history\n",
        "    if history is not None and hasattr(history, \"history\"):\n",
        "        with open(os.path.join(bundle_dir, \"history.json\"), \"w\") as f:\n",
        "            json.dump(history.history, f, indent=2)\n",
        "\n",
        "    # 5) samples from val\n",
        "    sample_dir = os.path.join(bundle_dir, \"samples\"); os.makedirs(sample_dir, exist_ok=True)\n",
        "    taken = 0\n",
        "    for xb, _ in val_ds.take(take_val):\n",
        "        yb = model(xb, training=False)\n",
        "        save_png01((xb.numpy()[0,...,0] + 0.5), os.path.join(sample_dir, f\"ae_input_val0.png\"))\n",
        "        save_png01((yb.numpy()[0,...,0] + 0.5), os.path.join(sample_dir, f\"ae_recon_val0.png\"))\n",
        "        taken += 1\n",
        "        if taken >= take_val: break\n",
        "\n",
        "    # 6) (optional) SavedModel\n",
        "    if also_savedmodel:\n",
        "        sm_dir = os.path.join(bundle_dir, \"saved_model\")\n",
        "        model.save(sm_dir)\n",
        "\n",
        "    # 7) latest symlink (best-effort)\n",
        "    latest = os.path.join(out_root, \"latest\")\n",
        "    try:\n",
        "        if os.path.islink(latest) or os.path.exists(latest):\n",
        "            os.remove(latest)\n",
        "        os.symlink(bundle_dir, latest)\n",
        "    except Exception:\n",
        "        with open(os.path.join(out_root, \"LATEST.txt\"), \"w\") as f:\n",
        "            f.write(bundle_dir)\n",
        "\n",
        "    print(f\"✅ Stage-1 bundle saved to: {bundle_dir}\")\n",
        "    return bundle_dir\n",
        "\n",
        "# -----------------------------\n",
        "# (I) Bit-accurate compression (print)\n",
        "# -----------------------------\n",
        "bits_per_top    = int(math.ceil(math.log2(NUM_EMBEDDINGS_TOP)))       # 256 -> 8\n",
        "bits_per_bottom = int(math.ceil(math.log2(NUM_EMBEDDINGS_BOTTOM)))    # 256 -> 8\n",
        "tokens_top      = TOP_GRID * TOP_GRID          # 1024\n",
        "tokens_bottom   = BOTTOM_GRID * BOTTOM_GRID    # 4096\n",
        "latent_bits     = tokens_top * bits_per_top + tokens_bottom * bits_per_bottom\n",
        "orig_bits       = IMAGE_SIZE * IMAGE_SIZE * 8\n",
        "bpp             = latent_bits / (IMAGE_SIZE * IMAGE_SIZE)\n",
        "ratio           = orig_bits / latent_bits\n",
        "\n",
        "print(\"=\"*66)\n",
        "print(f\"Bit-accurate compression (Top+Bottom): {ratio:.1f}×  (~{bpp:.5f} bpp)\")\n",
        "print(f\" - tokens: top {tokens_top} + bottom {tokens_bottom} = {tokens_top + tokens_bottom}\")\n",
        "print(f\" - bits/token: top {bits_per_top}, bottom {bits_per_bottom}\")\n",
        "print(f\" - commitment(bottom) = {COMMITMENT_COST_BOTTOM}\")\n",
        "print(\"=\"*66)\n",
        "\n",
        "# -----------------------------\n",
        "# (J) Train with auto-save best → load best → export bundle\n",
        "# -----------------------------\n",
        "EPOCHS_WARMUP = int(globals().get('EPOCHS_WARMUP', 50))\n",
        "\n",
        "compile_warmup(model)\n",
        "\n",
        "out_root = \"/content/drive/MyDrive/vqvae_stage1_runs\"\n",
        "run_stamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "run_dir   = os.path.join(out_root, run_stamp)\n",
        "cbs = make_stage1_callbacks(\n",
        "    run_dir,\n",
        "    monitor_primary=\"val_ssim_metric\",\n",
        "    monitor_secondary=\"val_loss\",\n",
        "    patience=8\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_WARMUP,\n",
        "    callbacks=cbs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "best_ckpt = os.path.join(run_dir, \"stage1_best_by_vloss.weights.h5\")\n",
        "model.load_weights(best_ckpt)\n",
        "print(f\"Loaded best weights: {best_ckpt}\")\n",
        "\n",
        "\n",
        "bundle_dir = export_stage1_bundle(\n",
        "    model,\n",
        "    out_root=out_root,\n",
        "    run_dir=run_stamp,\n",
        "    history=history,\n",
        "    also_savedmodel=False\n",
        ")\n",
        "print(\"Exported:\", bundle_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbO2j-FkPsFc"
      },
      "source": [
        "## 3) Two-Level VQ-VAE + Transformer Prior (1024×)\n",
        "\n",
        "You can also use PixelCNN to train a prior or bellow - but may need to have some fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr8zVtmJx0rS"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Prior Transformer — 省显存可跑版（混合精度 / 256×6 / logits回float32 / batch=1）\n",
        "# ============================================\n",
        "import os, json, math, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# -------- 0) 显存 & 精度设置（必须在建模前）---------\n",
        "# 可选：让 TensorFlow 对 GPU 显存按需增长，避免一次性占满\n",
        "try:\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if gpus:\n",
        "        for g in gpus:\n",
        "            tf.config.experimental.set_memory_growth(g, True)\n",
        "        print(\"✅ Enabled GPU memory growth\")\n",
        "except Exception as e:\n",
        "    print(\"⚠️ memory growth set failed:\", e)\n",
        "\n",
        "# 开启混合精度：注意必须在任何模型/层创建之前调用\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "print(\"✅ Mixed precision policy:\", mixed_precision.global_policy())\n",
        "\n",
        "# -------------------------\n",
        "# \n",
        "# -------------------------\n",
        "EXT_DIR         = \"/content/drive/MyDrive/vqvae_stage2_tokens\"\n",
        "INDEX_EXT       = os.path.join(EXT_DIR, \"index_ext.json\")\n",
        "ADAPTER_WEIGHTS = \"/content/drive/MyDrive/vqvae_stage2_adapter/adapter_best.weights.h5\"\n",
        "PRIOR_OUT       = \"/content/drive/MyDrive/vqvae_stage2_prior_full_mem\"\n",
        "os.makedirs(PRIOR_OUT, exist_ok=True)\n",
        "\n",
        "with open(INDEX_EXT, \"r\") as f:\n",
        "    NPZ_LIST = json.load(f)\n",
        "print(\"总样本:\", len(NPZ_LIST))\n",
        "\n",
        "# 尺寸/词表（保持与你前面一致）\n",
        "TOP_H, TOP_W  = 32, 32\n",
        "BOT_H, BOT_W  = 64, 64\n",
        "TGT_LEN       = BOT_H * BOT_W\n",
        "VBOT          = 256            # bottom 词表\n",
        "VBOT_PLUS     = VBOT + 1       # 0:BOS, 1..256 对应真实 0..255\n",
        "VTOP          = 256\n",
        "Hb            = 128            # bottom 隐特征通道数\n",
        "\n",
        "# 批量设 1（配合混合精度，一般就能稳）\n",
        "BATCH_SIZE    = 1\n",
        "VAL_RATIO     = 0.10\n",
        "\n",
        "# -------------------------\n",
        "# 数据集：((bot_in, top_in, cond_mask), target_bot)\n",
        "# -------------------------\n",
        "def _load_npz(path_b):\n",
        "    p = path_b.numpy().decode(\"utf-8\")\n",
        "    d = np.load(p, allow_pickle=False)\n",
        "    top = d[\"top32_only\"].astype(np.int32)        # [32,32]\n",
        "    bot = d[\"bot64\"].astype(np.int32)             # [64,64]\n",
        "    return top, bot\n",
        "\n",
        "def parse_for_prior(path):\n",
        "    top, bot = tf.py_function(_load_npz, [path], [tf.int32, tf.int32])\n",
        "    top.set_shape((TOP_H, TOP_W))\n",
        "    bot.set_shape((BOT_H, BOT_W))\n",
        "    cond_mask = tf.ones((), dtype=tf.float32)     # 训练期默认全条件=1.0；采样时可切换CFG\n",
        "    return ((bot, top, cond_mask), bot)\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "def make_ds(files, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(files)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(min(20000, len(files)), reshuffle_each_iteration=True)\n",
        "    ds = ds.map(parse_for_prior, num_parallel_calls=AUTO)\n",
        "    ds = ds.batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTO)\n",
        "    return ds\n",
        "\n",
        "n_total   = len(NPZ_LIST)\n",
        "n_val     = max(1, int(n_total * VAL_RATIO))\n",
        "VAL_FILES = NPZ_LIST[:n_val]\n",
        "TRN_FILES = NPZ_LIST[n_val:]\n",
        "\n",
        "train_ds = make_ds(TRN_FILES, shuffle=True)\n",
        "val_ds   = make_ds(VAL_FILES, shuffle=False)\n",
        "print(f\"Train: {len(TRN_FILES)} | Val: {len(VAL_FILES)}\")\n",
        "\n",
        "# -------------------------\n",
        "# Strong Adapter（复用你已训练的；冻结）\n",
        "# -------------------------\n",
        "D_EMB  = 256   # 与 prior 的 d_model 对齐即可\n",
        "HEADS  = 8\n",
        "\n",
        "def GN(groups=16):\n",
        "    # 有 GroupNorm 用 GN；没有就退化到 LayerNorm\n",
        "    try:\n",
        "        return layers.GroupNormalization(groups=groups)\n",
        "    except Exception:\n",
        "        return layers.LayerNormalization(axis=-1)\n",
        "\n",
        "def LambdaReshape_HW_to_seq(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H*W, tf.shape(t)[-1]]))\n",
        "\n",
        "def LambdaReshape_seq_to_HW(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H, W, tf.shape(t)[-1]]))\n",
        "\n",
        "def PixelShuffleLayer(r=2):\n",
        "    return layers.Lambda(lambda t: tf.nn.depth_to_space(t, r))\n",
        "\n",
        "def ResConv(x, ch, gn_groups=16):\n",
        "    s = x\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x); x = GN(gn_groups)(x); x = layers.Activation('gelu')(x)\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x); x = GN(gn_groups)(x)\n",
        "    return layers.Activation('gelu')(layers.Add()([x, s]))\n",
        "\n",
        "def MHSA_2d_32(x, heads=HEADS, D=D_EMB):\n",
        "    to_seq = LambdaReshape_HW_to_seq(32, 32); to_hw = LambdaReshape_seq_to_HW(32, 32)\n",
        "    seq = to_seq(x)\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D//heads, output_shape=D)\n",
        "    y = att(seq, seq, use_causal_mask=False)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, seq]))\n",
        "    y2= layers.Dense(4*D, activation='gelu')(y); y2 = layers.Dense(D)(y2)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw(y)\n",
        "\n",
        "def CrossAttn_2d_64x32(q_64, kv_32, heads=HEADS, D_kv=D_EMB, C_q=None):\n",
        "    if C_q is None: C_q = q_64.shape[-1]\n",
        "    to_seq64 = LambdaReshape_HW_to_seq(64, 64); to_hw64 = LambdaReshape_seq_to_HW(64, 64)\n",
        "    q = to_seq64(q_64)\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D_kv//heads, output_shape=int(C_q))\n",
        "    y = att(q, kv_32)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, q]))\n",
        "    y2= layers.Dense(int(4*C_q), activation='gelu')(y); y2 = layers.Dense(int(C_q))(y2)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw64(y)\n",
        "\n",
        "def build_strong_adapter(vocab_top=VTOP, D=D_EMB, Hb=Hb, heads=HEADS):\n",
        "    t_idx = keras.Input((32,32), dtype='int32', name='toponly_idx_32x32')\n",
        "    F32 = layers.Embedding(vocab_top, D, name='emb_toponly')(t_idx)\n",
        "    x32 = ResConv(F32, D); x32 = ResConv(x32, D); x32 = MHSA_2d_32(x32, heads=heads, D=D)\n",
        "    kv_seq = LambdaReshape_HW_to_seq(32, 32)(x32)\n",
        "    pos_ids = tf.constant([[i for i in range(32*32)]], dtype=tf.int32)\n",
        "    pos_emb = layers.Embedding(32*32, D, name='pos_kv_emb')(pos_ids)\n",
        "    kv_seq  = layers.Add()([kv_seq, pos_emb])\n",
        "    up64 = layers.Conv2D(D//2*4, 3, padding='same')(x32); up64 = PixelShuffleLayer(r=2)(up64)\n",
        "    up64 = ResConv(up64, D//2); up64 = ResConv(up64, D//2)\n",
        "    up64 = CrossAttn_2d_64x32(up64, kv_seq, heads=heads, D_kv=D, C_q=D//2)\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=2, activation='gelu')(up64)\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=3, activation='gelu')(y)\n",
        "    y = layers.LayerNormalization(axis=-1)(y)\n",
        "    out = layers.Conv2D(Hb, 1, padding='same', name='t_up_hat')(y)  # [B,64,64,Hb]\n",
        "    return keras.Model(t_idx, out, name='AdapterStrong')\n",
        "\n",
        "adapter = build_strong_adapter()\n",
        "_ = adapter(tf.zeros([1,32,32], dtype=tf.int32))\n",
        "adapter.load_weights(ADAPTER_WEIGHTS)\n",
        "adapter.trainable = False\n",
        "print(\"✅ Adapter loaded & frozen:\", ADAPTER_WEIGHTS)\n",
        "\n",
        "# -------------------------\n",
        "# Prior：二维感知 + 条件注入 + CFG 支持（省显存版）\n",
        "# 输出：[B,64,64,256]（logits），且 logits 为 float32（便于稳定计算 CE）\n",
        "# -------------------------\n",
        "def build_prior_transformer(\n",
        "    d_model=256,            # ★ 512->256\n",
        "    n_heads=8,\n",
        "    n_layers=6,             # ★ 10->6\n",
        "    ff_mult=4, dropout=0.1, cond_dropout=0.1\n",
        "):\n",
        "    # ---- Inputs ----\n",
        "    bot_gt = keras.Input(shape=(BOT_H, BOT_W), dtype='int32', name='bot_gt_64x64')\n",
        "    top_in = keras.Input(shape=(TOP_H, TOP_W), dtype='int32', name='top_only_32x32')\n",
        "    cond_m = keras.Input(shape=(), dtype='float32', name='cond_mask')  # 训练=1.0；CFG 时切 0/1\n",
        "\n",
        "    # ---- teacher-forcing: 右移 + BOS，再 +1 偏移到 1..256 ----\n",
        "    tgt = layers.Reshape((TGT_LEN,), name=\"flat_bot\")(bot_gt)  # [B,T]\n",
        "    def _shift_right_plus1(t):\n",
        "        z = tf.zeros_like(t[:, :1])\n",
        "        t_in = tf.concat([z, t[:, :-1]], axis=1)\n",
        "        return t_in + 1\n",
        "    tgt_in = layers.Lambda(_shift_right_plus1, name=\"shift_right_plus1\")(tgt)  # [B,T] ∈ [1..256]\n",
        "\n",
        "    # ---- bottom token embedding ----\n",
        "    x = layers.Embedding(VBOT_PLUS, d_model, name='emb_bot')(tgt_in)  # [B,T,D]\n",
        "\n",
        "    # ---- 2D + 绝对位置（常量索引，避免图内 tf.* 触发符号问题）----\n",
        "    row_idx_np = np.repeat(np.arange(BOT_H)[:, None], BOT_W, axis=1).reshape(1, TGT_LEN).astype('int32')\n",
        "    col_idx_np = np.repeat(np.arange(BOT_W)[None, :], BOT_H, axis=0).reshape(1, TGT_LEN).astype('int32')\n",
        "    abs_idx_np = np.arange(TGT_LEN)[None, :].astype('int32')\n",
        "\n",
        "    row_pe = layers.Embedding(BOT_H, d_model, name='row_pe')(tf.constant(row_idx_np))\n",
        "    col_pe = layers.Embedding(BOT_W, d_model, name='col_pe')(tf.constant(col_idx_np))\n",
        "    abs_pe = layers.Embedding(TGT_LEN, d_model, name='abs_pe')(tf.constant(abs_idx_np))\n",
        "    x = layers.Add(name=\"add_pos_pe\")([x, row_pe, col_pe, abs_pe])  # [B,T,D]\n",
        "\n",
        "    # ---- Adapter 条件 ----\n",
        "    t_up = adapter(top_in)                         # [B,64,64,Hb]\n",
        "    t_up = layers.Dropout(cond_dropout)(t_up)      # train only\n",
        "    tproj = layers.Conv2D(d_model, 1, padding='same', name='tup_proj')(t_up)  # [B,64,64,D]\n",
        "    tproj = layers.Reshape((TGT_LEN, d_model))(tproj)                          # [B,T,D]\n",
        "    cond_scale = layers.Lambda(lambda c: tf.reshape(c, (-1, 1, 1)), name=\"cond_scale\")(cond_m)\n",
        "    tproj = layers.Multiply(name=\"tproj_mask\")([tproj, cond_scale])\n",
        "    x = layers.Add(name=\"add_tproj\")([x, tproj])   # [B,T,D]\n",
        "\n",
        "    # ---- Top-only KV 记忆 ----\n",
        "    top_seq = layers.Reshape((TOP_H*TOP_W,), name=\"flat_top\")(top_in)       # [B,1024]\n",
        "    mem = layers.Embedding(VTOP, d_model, name='emb_top')(top_seq)          # [B,1024,D]\n",
        "    top_pos_idx_np = np.arange(TOP_H*TOP_W)[None, :].astype('int32')\n",
        "    mem_pos = layers.Embedding(TOP_H*TOP_W, d_model, name='pos_top')(tf.constant(top_pos_idx_np))\n",
        "    mem = layers.Add(name=\"add_top_pos\")([mem, mem_pos])                    # [B,1024,D]\n",
        "    mem = layers.Dropout(cond_dropout)(mem)                                 # train only\n",
        "    cond_scale_mem = layers.Lambda(lambda c: tf.reshape(c, (-1, 1, 1)), name=\"cond_scale_mem\")(cond_m)\n",
        "    mem = layers.Multiply(name=\"mem_mask\")([mem, cond_scale_mem])\n",
        "\n",
        "    # ---- Transformer Decoder blocks（Pre-LN）----\n",
        "    for i in range(n_layers):\n",
        "        ln1 = layers.LayerNormalization(name=f'ln1_{i}')(x)\n",
        "        sa  = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model//n_heads,\n",
        "                                        dropout=dropout, name=f'self_attn_{i}')\n",
        "        x_sa = sa(ln1, ln1, use_causal_mask=True)\n",
        "        x = layers.Add(name=f'resid_sa_{i}')([x, x_sa])\n",
        "\n",
        "        ln2 = layers.LayerNormalization(name=f'ln2_{i}')(x)\n",
        "        ca  = layers.MultiHeadAttention(num_heads=n_heads, key_dim=d_model//n_heads,\n",
        "                                        dropout=dropout, name=f'cross_attn_{i}')\n",
        "        x_ca = ca(ln2, mem)\n",
        "        x = layers.Add(name=f'resid_ca_{i}')([x, x_ca])\n",
        "\n",
        "        ln3 = layers.LayerNormalization(name=f'ln3_{i}')(x)\n",
        "        y = layers.Dense(ff_mult*d_model, activation='gelu', name=f'ffn_{i}_fc1')(ln3)\n",
        "        y = layers.Dropout(dropout, name=f'ffn_{i}_drop')(y)\n",
        "        y = layers.Dense(d_model, name=f'ffn_{i}_fc2')(y)\n",
        "        x = layers.Add(name=f'resid_ffn_{i}')([x, y])\n",
        "\n",
        "    x = layers.LayerNormalization(name='ln_out')(x)\n",
        "    # ★ logits 显式设为 float32，避免半精度下 CE 数值不稳\n",
        "    logits_seq = layers.Dense(VBOT, name='logits', dtype='float32')(x)                 # [B,4096,256] fp32\n",
        "    logits_4d  = layers.Reshape((BOT_H, BOT_W, VBOT), name='logits_4d')(logits_seq)    # [B,64,64,256]\n",
        "    return keras.Model(inputs=[bot_gt, top_in, cond_m], outputs=logits_4d, name='PriorTransformerFull')\n",
        "\n",
        "prior = build_prior_transformer(\n",
        "    d_model=256, n_heads=8, n_layers=6, ff_mult=4, dropout=0.1, cond_dropout=0.1\n",
        ")\n",
        "prior.summary(line_length=140)\n",
        "\n",
        "# -------------------------\n",
        "# 训练设置：AdamW + warmup+cosine / 梯度裁剪 / 指标\n",
        "# -------------------------\n",
        "class WarmupCosine(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, base_lr, warmup_steps, total_steps, name=None):\n",
        "        super().__init__()\n",
        "        self.base_lr = float(base_lr)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.total_steps  = int(total_steps)\n",
        "        self.name = name or \"WarmupCosine\"\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        wu = tf.cast(self.warmup_steps, tf.float32)\n",
        "        ts = tf.cast(tf.maximum(1, self.total_steps), tf.float32)\n",
        "        base = tf.cast(self.base_lr, tf.float32)\n",
        "        lr_wu = base * tf.minimum(1.0, step / tf.maximum(1.0, wu))\n",
        "        progress = tf.clip_by_value((step - wu) / tf.maximum(1.0, ts - wu), 0.0, 1.0)\n",
        "        lr_cos = 0.5 * base * (1.0 + tf.cos(tf.constant(math.pi, tf.float32) * progress))\n",
        "        return tf.where(step < wu, lr_wu, lr_cos)\n",
        "    def get_config(self):\n",
        "        return {\"base_lr\": self.base_lr, \"warmup_steps\": self.warmup_steps,\n",
        "                \"total_steps\": self.total_steps, \"name\": self.name}\n",
        "\n",
        "steps_per_epoch = max(1, math.ceil(len(TRN_FILES) / BATCH_SIZE))\n",
        "total_steps     = steps_per_epoch * 30\n",
        "sched = WarmupCosine(base_lr=2e-4, warmup_steps=min(2000, steps_per_epoch*3), total_steps=total_steps)\n",
        "\n",
        "try:\n",
        "    opt = keras.optimizers.AdamW(learning_rate=sched, weight_decay=1e-4, clipnorm=1.0)\n",
        "except Exception:\n",
        "    opt = keras.optimizers.Adam(learning_rate=sched, clipnorm=1.0)\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "acc_fn  = keras.metrics.SparseCategoricalAccuracy(name=\"acc\")\n",
        "def ppl_metric(y_true, y_pred):\n",
        "    ce = keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "    return tf.exp(tf.reduce_mean(ce))\n",
        "\n",
        "prior.compile(optimizer=opt, loss=loss_fn, metrics=[acc_fn, ppl_metric])\n",
        "\n",
        "cbs = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(PRIOR_OUT, \"prior_best.weights.h5\"),\n",
        "        save_weights_only=True, save_best_only=True,\n",
        "        monitor=\"val_loss\", mode=\"min\", verbose=1\n",
        "    ),\n",
        "    keras.callbacks.CSVLogger(os.path.join(PRIOR_OUT, \"prior_log.csv\")),\n",
        "    keras.callbacks.TensorBoard(log_dir=os.path.join(PRIOR_OUT, \"tb\")),\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
        "]\n",
        "\n",
        "# -------------------------\n",
        "# 开训\n",
        "# -------------------------\n",
        "hist = prior.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=cbs, verbose=1)\n",
        "prior.save_weights(os.path.join(PRIOR_OUT, \"prior_final.weights.h5\"))\n",
        "print(\"✅ Prior saved to:\", PRIOR_OUT)\n",
        "\n",
        "# =========================\n",
        "# （可选）梯度累积版本：需要更大模型或更大等效batch可启用\n",
        "# 使用方法：\n",
        "#    1) 取消以下类与两行替换的注释\n",
        "#    2) 用 AccumulateModel 包装 prior 再 compile & fit\n",
        "# =========================\n",
        "# class AccumulateModel(keras.Model):\n",
        "#     def __init__(self, acc_steps=4, **kwargs):\n",
        "#         super().__init__(**kwargs)\n",
        "#         self.acc_steps = acc_steps\n",
        "#         self._grad_accum = None\n",
        "#         self._step = 0\n",
        "#     def train_step(self, data):\n",
        "#         (bot_in, top_in, cond_m), y = data\n",
        "#         with tf.GradientTape() as tape:\n",
        "#             y_pred = self([bot_in, top_in, cond_m], training=True)\n",
        "#             loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
        "#         grads = tape.gradient(loss, self.trainable_variables)\n",
        "#         if self._grad_accum is None:\n",
        "#             self._grad_accum = [tf.zeros_like(g) for g in grads]\n",
        "#         self._grad_accum = [ga + g for ga, g in zip(self._grad_accum, grads)]\n",
        "#         self._step += 1\n",
        "#         if self._step % self.acc_steps == 0:\n",
        "#             self.optimizer.apply_gradients(zip(self._grad_accum, self.trainable_variables))\n",
        "#             self._grad_accum = None\n",
        "#         self.compiled_metrics.update_state(y, y_pred)\n",
        "#         logs = {m.name: m.result() for m in self.metrics}\n",
        "#         logs.update({\"loss\": loss})\n",
        "#         return logs\n",
        "\n",
        "# # 使用梯度累积：\n",
        "# prior = AccumulateModel(acc_steps=4, inputs=prior.inputs, outputs=prior.outputs)\n",
        "# prior.compile(optimizer=opt, loss=loss_fn, metrics=[acc_fn, ppl_metric])\n",
        "# hist = prior.fit(train_ds, validation_data=val_ds, epochs=30, callbacks=cbs, verbose=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus - Train a Adapter\n",
        "\n",
        "For image generation you can also train a Adapter to speed-up the decoding section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d6VgyXWnILs"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Strong Adapter (32x32 indices -> 64x64xHb) — Keras-only\n",
        "# =============================\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "Hb   = 128   # two-level 的 bottom hidden 通道\n",
        "VTOP = 256   # top-only 词表大小\n",
        "D    = 256   # token embedding 维度\n",
        "HEADS = 8\n",
        "\n",
        "# ---- small helpers (Keras-only) ----\n",
        "def GN(groups=16):\n",
        "    # 兼容：若没有 GroupNormalization 就退化成 LayerNormalization\n",
        "    try:\n",
        "        return layers.GroupNormalization(groups=groups)\n",
        "    except Exception:\n",
        "        return layers.LayerNormalization(axis=-1)\n",
        "\n",
        "def LambdaReshape_HW_to_seq(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H*W, tf.shape(t)[-1]]))\n",
        "\n",
        "def LambdaReshape_seq_to_HW(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H, W, tf.shape(t)[-1]]))\n",
        "\n",
        "def PixelShuffleLayer(r=2):\n",
        "    return layers.Lambda(lambda t: tf.nn.depth_to_space(t, r))\n",
        "\n",
        "def ResConv(x, ch, gn_groups=16):\n",
        "    skip = x\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x)\n",
        "    x = GN(gn_groups)(x)\n",
        "    x = layers.Activation('gelu')(x)\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x)\n",
        "    x = GN(gn_groups)(x)\n",
        "    x = layers.Activation('gelu')(layers.Add()([x, skip]))\n",
        "    return x\n",
        "\n",
        "def MHSA_2d_32(x, heads=HEADS, D=D):\n",
        "    # x: [B,32,32,D]  -> attn -> [B,32,32,D]\n",
        "    to_seq = LambdaReshape_HW_to_seq(32, 32)\n",
        "    to_hw  = LambdaReshape_seq_to_HW(32, 32)\n",
        "    seq = to_seq(x)\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D//heads, output_shape=D)\n",
        "    y = att(seq, seq, use_causal_mask=False)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, seq]))\n",
        "    y2 = layers.Dense(4*D, activation='gelu')(y)\n",
        "    y2 = layers.Dense(D)(y2)\n",
        "    y  = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw(y)\n",
        "\n",
        "def CrossAttn_2d_64x32(q_64, kv_32, heads=HEADS, D_kv=D, C_q=None):\n",
        "    # q_64: [B,64,64,Cq]   kv_32: [B,32*32,D]\n",
        "    # 输出形状与 q_64 一致\n",
        "    if C_q is None:\n",
        "        C_q = q_64.shape[-1]  # 这里是 D//2\n",
        "    to_seq64 = LambdaReshape_HW_to_seq(64, 64)\n",
        "    to_hw64  = LambdaReshape_seq_to_HW(64, 64)\n",
        "    q = to_seq64(q_64)                              # [B,4096,Cq]\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D_kv//heads, output_shape=int(C_q))\n",
        "    y = att(q, kv_32)                               # [B,4096,Cq]\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, q]))\n",
        "    y2= layers.Dense(int(4*C_q), activation='gelu')(y)\n",
        "    y2= layers.Dense(int(C_q))(y2)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw64(y)\n",
        "\n",
        "def build_strong_adapter(vocab_top=VTOP, D=D, Hb=Hb, heads=HEADS):\n",
        "    # 输入：32x32 的 top-only 索引\n",
        "    t_idx = keras.Input((32,32), dtype='int32', name='toponly_idx_32x32')\n",
        "\n",
        "    # 32×32 token embedding + 两个残差 + 自注意力\n",
        "    F32 = layers.Embedding(vocab_top, D, name='emb_toponly')(t_idx)  # [B,32,32,D]\n",
        "    x32 = ResConv(F32, D); x32 = ResConv(x32, D)\n",
        "    x32 = MHSA_2d_32(x32, heads=heads, D=D)\n",
        "\n",
        "    # 作为 KV 的序列（加绝对位置）\n",
        "    kv_seq = LambdaReshape_HW_to_seq(32, 32)(x32)                    # [B,1024,D]\n",
        "    pos_ids = tf.constant([[i for i in range(32*32)]], dtype=tf.int32)  # 常量OK\n",
        "    pos_emb = layers.Embedding(32*32, D, name='pos_kv_emb')(pos_ids)    # [1,1024,D]\n",
        "    kv_seq  = layers.Add()([kv_seq, pos_emb])                          # broadcast 到 B\n",
        "\n",
        "    # 上采样到 64×64（像素重排），通道减半 -> Cq = D//2\n",
        "    up64 = layers.Conv2D(D//2*4, 3, padding='same')(x32)  # 预卷积后 PixelShuffle\n",
        "    up64 = PixelShuffleLayer(r=2)(up64)                   # [B,64,64,D//2]\n",
        "    up64 = ResConv(up64, D//2); up64 = ResConv(up64, D//2)\n",
        "\n",
        "    # 交叉注意力：Q=64×64×(D//2)，K/V=kv_seq(32×32×D)\n",
        "    up64 = CrossAttn_2d_64x32(up64, kv_seq, heads=heads, D_kv=D, C_q=D//2)\n",
        "\n",
        "    # 空洞卷积细化 + LN + 1×1 到 Hb\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=2, activation='gelu')(up64)\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=3, activation='gelu')(y)\n",
        "    y = layers.LayerNormalization(axis=-1)(y)\n",
        "    out = layers.Conv2D(Hb, 1, padding='same', name='t_up_hat')(y)     # [B,64,64,Hb]\n",
        "\n",
        "    return keras.Model(t_idx, out, name='AdapterStrong')\n",
        "\n",
        "# --- build & check ---\n",
        "adapter = build_strong_adapter()\n",
        "_ = adapter(tf.zeros([1,32,32], dtype=tf.int32))  # build\n",
        "adapter.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltYOPvYvnIAu"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# Adapter 训练（端到端可运行版）\n",
        "# - 读取 tok_ext_*.npz  -> 训练 Strong Adapter\n",
        "# - 修复回调文件名后缀  -> *.weights.h5\n",
        "# ============================================\n",
        "import os, json, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# ---- 路径 & 常量（按需改）----\n",
        "EXT_DIR   = \"/content/drive/MyDrive/vqvae_stage2_tokens\"   # 存 tok_ext_*.npz\n",
        "INDEX_EXT = os.path.join(EXT_DIR, \"index_ext.json\")\n",
        "assert os.path.exists(INDEX_EXT), \"缺少 index_ext.json（先跑 Stage-2 Prep++ 的导出脚本）\"\n",
        "\n",
        "with open(INDEX_EXT, \"r\") as f:\n",
        "    npz_list = json.load(f)\n",
        "print(\"总样本:\", len(npz_list))\n",
        "\n",
        "Hb    = 128   # two-level 的 bottom hidden 通道数\n",
        "VTOP  = 256   # top-only 词表\n",
        "D     = 256   # token embedding 维度\n",
        "HEADS = 8\n",
        "\n",
        "# -------------------------------\n",
        "# Dataset: 读 tok_ext_*.npz\n",
        "#   每个文件包含:\n",
        "#     - top32_only : [32,32] int\n",
        "#     - t_up_gt    : [64,64,Hb] float\n",
        "# -------------------------------\n",
        "def _load_npz(path_bytes):\n",
        "    path = path_bytes.decode(\"utf-8\")\n",
        "    d = np.load(path, allow_pickle=False)\n",
        "    t_idx = d[\"top32_only\"].astype(np.int32)   # [32,32]\n",
        "    t_up  = d[\"t_up_gt\"].astype(np.float32)    # [64,64,Hb]\n",
        "    return t_idx, t_up\n",
        "\n",
        "def parse_fn(path):\n",
        "    t_idx, t_up = tf.numpy_function(_load_npz, [path], [tf.int32, tf.float32])\n",
        "    t_idx.set_shape((32, 32))\n",
        "    t_up.set_shape((64, 64, Hb))\n",
        "    # 产出 dict，便于后续 map 到 (x, y)\n",
        "    return {\"top_only\": t_idx, \"t_up\": t_up}\n",
        "\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "def make_ds(file_list, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(file_list)\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=min(20000, len(file_list)), reshuffle_each_iteration=True)\n",
        "    ds = ds.map(parse_fn, num_parallel_calls=AUTO)\n",
        "    ds = ds.batch(BATCH_SIZE, drop_remainder=False).prefetch(AUTO)\n",
        "    return ds\n",
        "\n",
        "# 划分 train/val\n",
        "VAL_RATIO = 0.10\n",
        "n_total   = len(npz_list)\n",
        "n_val     = max(1, int(n_total * VAL_RATIO))\n",
        "val_files   = npz_list[:n_val]\n",
        "train_files = npz_list[n_val:]\n",
        "print(f\"Train: {len(train_files)} | Val: {len(val_files)}\")\n",
        "\n",
        "train_ds = make_ds(train_files, shuffle=True)\n",
        "val_ds   = make_ds(val_files, shuffle=False)\n",
        "\n",
        "# -------------------------------\n",
        "# Strong Adapter (与你上条消息一致的 Keras-only 版本)\n",
        "# -------------------------------\n",
        "def GN(groups=16):\n",
        "    try:\n",
        "        return layers.GroupNormalization(groups=groups)\n",
        "    except Exception:\n",
        "        return layers.LayerNormalization(axis=-1)\n",
        "\n",
        "def LambdaReshape_HW_to_seq(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H*W, tf.shape(t)[-1]]))\n",
        "\n",
        "def LambdaReshape_seq_to_HW(H, W):\n",
        "    return layers.Lambda(lambda t: tf.reshape(t, [tf.shape(t)[0], H, W, tf.shape(t)[-1]]))\n",
        "\n",
        "def PixelShuffleLayer(r=2):\n",
        "    return layers.Lambda(lambda t: tf.nn.depth_to_space(t, r))\n",
        "\n",
        "def ResConv(x, ch, gn_groups=16):\n",
        "    skip = x\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x)\n",
        "    x = GN(gn_groups)(x)\n",
        "    x = layers.Activation('gelu')(x)\n",
        "    x = layers.Conv2D(ch, 3, padding='same')(x)\n",
        "    x = GN(gn_groups)(x)\n",
        "    x = layers.Activation('gelu')(layers.Add()([x, skip]))\n",
        "    return x\n",
        "\n",
        "def MHSA_2d_32(x, heads=HEADS, D=D):\n",
        "    to_seq = LambdaReshape_HW_to_seq(32, 32)\n",
        "    to_hw  = LambdaReshape_seq_to_HW(32, 32)\n",
        "    seq = to_seq(x)\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D//heads, output_shape=D)\n",
        "    y = att(seq, seq, use_causal_mask=False)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, seq]))\n",
        "    y2 = layers.Dense(4*D, activation='gelu')(y)\n",
        "    y2 = layers.Dense(D)(y2)\n",
        "    y  = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw(y)\n",
        "\n",
        "def CrossAttn_2d_64x32(q_64, kv_32, heads=HEADS, D_kv=D, C_q=None):\n",
        "    if C_q is None:\n",
        "        C_q = q_64.shape[-1]\n",
        "    to_seq64 = LambdaReshape_HW_to_seq(64, 64)\n",
        "    to_hw64  = LambdaReshape_seq_to_HW(64, 64)\n",
        "    q = to_seq64(q_64)                              # [B,4096,Cq]\n",
        "    att = layers.MultiHeadAttention(num_heads=heads, key_dim=D_kv//heads, output_shape=int(C_q))\n",
        "    y = att(q, kv_32)                               # [B,4096,Cq]\n",
        "    y = layers.LayerNormalization()(layers.Add()([y, q]))\n",
        "    y2= layers.Dense(int(4*C_q), activation='gelu')(y)\n",
        "    y2= layers.Dense(int(C_q))(y2)\n",
        "    y = layers.LayerNormalization()(layers.Add()([y2, y]))\n",
        "    return to_hw64(y)\n",
        "\n",
        "def build_strong_adapter(vocab_top=VTOP, D=D, Hb=Hb, heads=HEADS):\n",
        "    t_idx = keras.Input((32,32), dtype='int32', name='toponly_idx_32x32')\n",
        "\n",
        "    F32 = layers.Embedding(vocab_top, D, name='emb_toponly')(t_idx)  # [B,32,32,D]\n",
        "    x32 = ResConv(F32, D); x32 = ResConv(x32, D)\n",
        "    x32 = MHSA_2d_32(x32, heads=heads, D=D)\n",
        "\n",
        "    # KV 序列 + 绝对位置\n",
        "    kv_seq = LambdaReshape_HW_to_seq(32, 32)(x32)                     # [B,1024,D]\n",
        "    pos_ids = tf.constant([[i for i in range(32*32)]], dtype=tf.int32)\n",
        "    pos_emb = layers.Embedding(32*32, D, name='pos_kv_emb')(pos_ids)  # [1,1024,D]\n",
        "    kv_seq  = layers.Add()([kv_seq, pos_emb])                         # broadcast 到 B\n",
        "\n",
        "    # 上采样到 64×64\n",
        "    up64 = layers.Conv2D(D//2*4, 3, padding='same')(x32)\n",
        "    up64 = PixelShuffleLayer(r=2)(up64)                               # [B,64,64,D//2]\n",
        "    up64 = ResConv(up64, D//2); up64 = ResConv(up64, D//2)\n",
        "    up64 = CrossAttn_2d_64x32(up64, kv_seq, heads=heads, D_kv=D, C_q=D//2)\n",
        "\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=2, activation='gelu')(up64)\n",
        "    y = layers.Conv2D(D//2, 3, padding='same', dilation_rate=3, activation='gelu')(y)\n",
        "    y = layers.LayerNormalization(axis=-1)(y)\n",
        "    out = layers.Conv2D(Hb, 1, padding='same', name='t_up_hat')(y)    # [B,64,64,Hb]\n",
        "\n",
        "    return keras.Model(t_idx, out, name='AdapterStrong')\n",
        "\n",
        "adapter = build_strong_adapter()\n",
        "_ = adapter(tf.zeros([1,32,32], dtype=tf.int32))  # build\n",
        "\n",
        "# -------------------------------\n",
        "# 损失 & 训练\n",
        "# -------------------------------\n",
        "def cosine_loss(a, b, eps=1e-6):\n",
        "    a = tf.nn.l2_normalize(a, axis=-1)\n",
        "    b = tf.nn.l2_normalize(b, axis=-1)\n",
        "    return tf.reduce_mean(1.0 - tf.reduce_mean(a*b, axis=-1))\n",
        "\n",
        "def adapter_loss(y_true, y_pred):\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    cos = cosine_loss(y_true, y_pred)\n",
        "    return mse + 0.1 * cos\n",
        "\n",
        "try:\n",
        "    opt = tf.keras.optimizers.AdamW(3e-4, weight_decay=1e-4)\n",
        "except Exception:\n",
        "    opt = tf.keras.optimizers.Adam(3e-4)\n",
        "\n",
        "adapter.compile(optimizer=opt, loss=adapter_loss,\n",
        "                metrics=[keras.metrics.MeanSquaredError(name=\"mse\")])\n",
        "\n",
        "ADAPTER_OUT = \"/content/drive/MyDrive/vqvae_stage2_adapter\"\n",
        "os.makedirs(ADAPTER_OUT, exist_ok=True)\n",
        "cbs = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        os.path.join(ADAPTER_OUT, \"adapter_best.weights.h5\"),  # ← 修正为 .weights.h5\n",
        "        save_weights_only=True, save_best_only=True,\n",
        "        monitor=\"val_loss\", mode=\"min\", verbose=1),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=6, restore_best_weights=True),\n",
        "    tf.keras.callbacks.CSVLogger(os.path.join(ADAPTER_OUT, \"adapter_log.csv\")),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, verbose=1),\n",
        "]\n",
        "\n",
        "# (x, y) 二元组\n",
        "def map_xy(batch):\n",
        "    return batch[\"top_only\"], batch[\"t_up\"]\n",
        "\n",
        "EPOCHS = 30\n",
        "hist = adapter.fit(\n",
        "    train_ds.map(map_xy),\n",
        "    validation_data=val_ds.map(map_xy),\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=cbs,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"✅ 训练完成；最佳权重：\", os.path.join(ADAPTER_OUT, \"adapter_best.weights.h5\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YypBPrK5nH9m"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
